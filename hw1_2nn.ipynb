{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhdbsrlw/MLVU/blob/main/hw1_2nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCYO6dmGgefe"
      },
      "source": [
        "# Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er0RD438gRLm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f53e59ba-0688-4d8f-a777-bfb9d5dd2c69"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jfeql_8sgnKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2622d7b-b5f1-403d-bf05-be5e3f544155"
      },
      "source": [
        "\"\"\"\n",
        "Change directory to where this file is located\n",
        "\"\"\"\n",
        "%cd /content/drive/MyDrive/AI/VIP Lab/Homework 1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AI/VIP Lab/Homework 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPEoabX-hGCh"
      },
      "source": [
        "# Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyammZP8hI7P"
      },
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mnist.data_utils import load_data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLxTNOvI5NHD"
      },
      "source": [
        "#Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuQB6W2U5ZE2"
      },
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Do NOT modify this function\n",
        "    \"\"\"\n",
        "    return 1/(1+np.exp(-z))\n",
        "\n",
        "def softmax(X):\n",
        "    \"\"\"\n",
        "    Do NOT modify this function\n",
        "    \"\"\"\n",
        "    logit = np.exp(X-np.amax(X, axis=1, keepdims=True))\n",
        "    numer = logit\n",
        "    denom = np.sum(logit, axis=1, keepdims=True)\n",
        "    return numer/denom\n",
        "\n",
        "def load_batch(X, Y, batch_size, shuffle=True):\n",
        "    \"\"\"\n",
        "    Generates batches with the remainder dropped.\n",
        "\n",
        "    Do NOT modify this function\n",
        "    \"\"\"\n",
        "    if shuffle:\n",
        "        permutation = np.random.permutation(X.shape[0])\n",
        "        X = X[permutation, :]\n",
        "        Y = Y[permutation, :]\n",
        "    num_steps = int(X.shape[0])//batch_size\n",
        "    step = 0\n",
        "    while step<num_steps:\n",
        "        X_batch = X[batch_size*step:batch_size*(step+1)]\n",
        "        Y_batch = Y[batch_size*step:batch_size*(step+1)]\n",
        "        step+=1\n",
        "        yield X_batch, Y_batch"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsU8v_6khR30"
      },
      "source": [
        "#2-Layer Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA5udiGmhRb5"
      },
      "source": [
        "class TwoLayerNN:\n",
        "    \"\"\" a neural network with 2 layers \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, num_hiddens, num_classes):\n",
        "        \"\"\"\n",
        "        Do NOT modify this function.\n",
        "        \"\"\"\n",
        "        self.input_dim = input_dim\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.num_classes = num_classes\n",
        "        self.params = self.initialize_parameters(input_dim, num_hiddens, num_classes)\n",
        "\n",
        "    def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n",
        "        \"\"\"\n",
        "        initializes parameters with Xavier Initialization.\n",
        "\n",
        "        Question (a)\n",
        "        - refer to https://paperswithcode.com/method/xavier-initialization for Xavier initialization\n",
        "\n",
        "        Inputs\n",
        "        - input_dim\n",
        "        - num_hiddens: the number of hidden units\n",
        "        - num_classes: the number of classes\n",
        "        Returns\n",
        "        - params: a dictionary with the initialized parameters.\n",
        "        \"\"\"\n",
        "        np.random.seed(42)\n",
        "\n",
        "        W1 = np.random.randn(input_dim, num_hiddens) * np.sqrt(6 / (input_dim + num_hiddens))\n",
        "        # np.random.randn()함수는 표준정규분포표에서 임의의 수를 추출한뒤, 그 수를 바탕으로 하는 N차원 배열을 생성\n",
        "        b1 = np.zeros((1, num_hiddens))\n",
        "        W2 = np.random.randn(num_hiddens, num_classes) * np.sqrt(6 / (num_hiddens + num_classes))\n",
        "        b2 = np.zeros((1, num_classes))\n",
        "        params = {\n",
        "            \"W1\": W1,\n",
        "            \"b1\": b1,\n",
        "            \"W2\": W2,\n",
        "            \"b2\": b2\n",
        "        }\n",
        "        return params\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Defines and performs the feed forward step of a two-layer neural network.\n",
        "        Specifically, the network structue is given by\n",
        "\n",
        "          y = softmax(sigmoid(X W1 + b1) W2 + b2)\n",
        "\n",
        "        where X is the input matrix of shape (N, D), y is the class distribution matrix\n",
        "        of shape (N, C), N is the number of examples (either the entire dataset or\n",
        "        a mini-batch), D is the feature dimensionality, and C is the number of classes.\n",
        "\n",
        "        Question (b)\n",
        "        - ff_dict will be used to run backpropagation in backward method.\n",
        "\n",
        "        Inputs\n",
        "        - X: the input matrix of shape (N, D)\n",
        "\n",
        "        Returns\n",
        "        - y: the output of the model\n",
        "        - ff_dict: a dictionary with all the fully connected units and activations.\n",
        "        \"\"\"\n",
        "        W1, b1 = self.params['W1'], self.params['b1']\n",
        "        W2, b2 = self.params['W2'], self.params['b2']\n",
        "\n",
        "        # 첫번째 레이어\n",
        "        z1 = np.dot(X, W1) + b1\n",
        "        a1 = sigmoid(z1)\n",
        "\n",
        "        # 두번째 레이어\n",
        "        z2 = np.dot(a1, W2) + b2\n",
        "        y = softmax(z2)\n",
        "\n",
        "        ff_dict = {'Z1': z1, 'A1': a1, 'Z2': z2, 'Y': y}\n",
        "\n",
        "        return y, ff_dict\n",
        "\n",
        "\n",
        "\n",
        "    def backward(self, X, Y, ff_dict):\n",
        "        \"\"\"\n",
        "        Performs backpropagation over the two-layer neural network, and returns\n",
        "        a dictionary of gradients of all model parameters.\n",
        "\n",
        "        Question (c)\n",
        "\n",
        "        Inputs:\n",
        "         - X: the input matrix of shape (B, D), where B is the number of examples\n",
        "              in a mini-batch, D is the feature dimensionality.\n",
        "         - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n",
        "              where B is the number of examples in a mini-batch, C is the number\n",
        "              of classes.\n",
        "         - ff_dict: the dictionary containing all the fully connected units and\n",
        "              activations.\n",
        "\n",
        "        Returns:\n",
        "         - grads: a dictionary containing the gradients of corresponding weights and biases.\n",
        "        \"\"\"\n",
        "        A1 = ff_dict['A1']\n",
        "        Z1 = ff_dict['Z1']\n",
        "        Y_hat = ff_dict['Y']\n",
        "\n",
        "        B = X.shape[0]\n",
        "        dY = (Y_hat - Y) / B\n",
        "\n",
        "        # 두번째 레이어의 Gradients\n",
        "        dW2 = np.dot(A1.T, dY)\n",
        "        db2 = np.sum(dY, axis=0, keepdims=True)\n",
        "\n",
        "        dA1 = np.dot(dY, self.params['W2'].T)\n",
        "        dZ1 = dA1 * A1 * (1 - A1)\n",
        "\n",
        "        # 첫번째 레이어의 Gradients\n",
        "        dW1 = np.dot(X.T, dZ1)\n",
        "        db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "        # Gradients 저장\n",
        "        grads = {\n",
        "            'W1': dW1,\n",
        "            'b1': db1,\n",
        "            'W2': dW2,\n",
        "            'b2': db2\n",
        "        }\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def compute_loss(self, Y, Y_hat):\n",
        "        \"\"\"\n",
        "        Computes cross entropy loss.\n",
        "\n",
        "        Do NOT modify this function.\n",
        "\n",
        "        Inputs\n",
        "            Y: ground truth labels\n",
        "            Y_hat: predicted labels\n",
        "        Returns\n",
        "            loss:\n",
        "        \"\"\"\n",
        "        loss = -(1/Y.shape[0]) * np.sum(np.multiply(Y, np.log(Y_hat)))\n",
        "        return loss\n",
        "\n",
        "    def train(self, X, Y, X_val, Y_val, lr, n_epochs, batch_size, log_interval=1):\n",
        "        \"\"\"\n",
        "        Runs mini-batch gradient descent.\n",
        "\n",
        "        Do NOT Modify this method.\n",
        "\n",
        "        Inputs\n",
        "        - X\n",
        "        - Y\n",
        "        - X_val: validation data\n",
        "        - Y_Val: validation label\n",
        "        - lr: learning rate\n",
        "        - n_epochs: the number of epochs to run\n",
        "        - batch_size\n",
        "        - log_interval: the epoch interval to log the training progress.\n",
        "        \"\"\"\n",
        "        for epoch in range(n_epochs):\n",
        "            for X_batch, Y_batch in load_batch(X, Y, batch_size):\n",
        "                self.train_step(X_batch, Y_batch, batch_size, lr)\n",
        "            if epoch % log_interval==0:\n",
        "                Y_hat, ff_dict = self.forward(X)\n",
        "                train_loss = self.compute_loss(Y, Y_hat)\n",
        "                train_acc = self.evaluate(Y, Y_hat)\n",
        "                Y_hat, ff_dict = self.forward(X_val)\n",
        "                valid_loss = self.compute_loss(Y_val, Y_hat)\n",
        "                valid_acc = self.evaluate(Y_val, Y_hat)\n",
        "                print('epoch {:02} - train loss/acc: {:.3f} {:.3f}, valid loss/acc: {:.3f} {:.3f}'.\\\n",
        "                      format(epoch, train_loss, train_acc, valid_loss, valid_acc))\n",
        "\n",
        "    def train_step(self, X_batch, Y_batch, batch_size, lr):\n",
        "        \"\"\"\n",
        "        Updates the parameters using gradient descent.\n",
        "\n",
        "        Question (d)\n",
        "\n",
        "        Inputs\n",
        "        - X_batch\n",
        "        - Y_batch\n",
        "        - batch_size\n",
        "        - lr: learning rate\n",
        "        \"\"\"\n",
        "        # Forward\n",
        "        y_pred, ff_dict = self.forward(X_batch)\n",
        "\n",
        "        # Backward\n",
        "        grads = self.backward(X_batch, Y_batch, ff_dict)\n",
        "\n",
        "        # Update\n",
        "        self.params['W1'] -= lr * grads['W1']\n",
        "        self.params['b1'] -= lr * grads['b1']\n",
        "        self.params['W2'] -= lr * grads['W2']\n",
        "        self.params['b2'] -= lr * grads['b2']\n",
        "\n",
        "\n",
        "    def evaluate(self, Y, Y_hat):\n",
        "        \"\"\"\n",
        "        Computes classification accuracy.\n",
        "\n",
        "        Question (e)\n",
        "\n",
        "        Inputs\n",
        "        - Y: A numpy array of shape (N, C) containing the one-hot encoded labels,\n",
        "             where C is the number of classes.\n",
        "        - Y_hat: A numpy array of shape (N, C) containing the softmax outputs,\n",
        "             where C is the number of classes.\n",
        "\n",
        "        Returns\n",
        "            accuracy: the classification accuracy in float\n",
        "        \"\"\"\n",
        "        Y_pred = np.argmax(Y_hat, axis=1) # 소프트맥스 결과값을 클래스 라벨로 변환 (가장 높은 probability 의 클래스로 변환된다.)\n",
        "        Y_true = np.argmax(Y, axis=1) # 원-핫 인코딩 라벨을 클래스 라벨로 변환\n",
        "\n",
        "        accuracy = np.mean(Y_pred == Y_true) # 맞은 개수로 평균\n",
        "\n",
        "        return accuracy\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXM2lWhtDYC6"
      },
      "source": [
        "#Load MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48ooR6YIxYhC",
        "outputId": "5dc6fc3a-4c08-44f9-b05d-368f4ea4a785"
      },
      "source": [
        "X_train, Y_train, X_test, Y_test = load_data()\n",
        "\n",
        "idxs = np.arange(len(X_train))\n",
        "np.random.shuffle(idxs)\n",
        "split_idx = int(np.ceil(len(idxs)*0.8))\n",
        "X_valid, Y_valid = X_train[idxs[split_idx:]], Y_train[idxs[split_idx:]]\n",
        "X_train, Y_train = X_train[idxs[:split_idx]], Y_train[idxs[:split_idx]]\n",
        "print()\n",
        "print('Set validation data aside')\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Training labels shape: ', Y_train.shape)\n",
        "print('Validation data shape: ', X_valid.shape)\n",
        "print('Validation labels shape: ', Y_valid.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST data loaded:\n",
            "Training data shape: (60000, 784)\n",
            "Training labels shape: (60000, 10)\n",
            "Test data shape: (10000, 784)\n",
            "Test labels shape: (10000, 10)\n",
            "\n",
            "Set validation data aside\n",
            "Training data shape:  (48000, 784)\n",
            "Training labels shape:  (48000, 10)\n",
            "Validation data shape:  (12000, 784)\n",
            "Validation labels shape:  (12000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzw-D4Zr5xoi"
      },
      "source": [
        "#Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlnC_rerHPaN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06ec432d-7041-47e4-9362-f9a3d9c7453d"
      },
      "source": [
        "###\n",
        "# Question (f)\n",
        "# Tune the hyperparameters with validation data,\n",
        "# and print the results by running the lines below.\n",
        "###\n",
        "\n",
        "# 하이퍼파라미터 튜닝 실험 횟수 (Random Search 횟수)\n",
        "exp_num = 10\n",
        "for i in range(exp_num):\n",
        "\n",
        "  # 하이퍼파라미터 선언 및 범위 지정\n",
        "  num_hiddens = 2 ** np.random.randint(5, 9)\n",
        "  lr = 10 ** np.random.uniform(-6, -2)\n",
        "  n_epochs = 5 * np.random.randint(1,21)\n",
        "  batch_size = 2 ** np.random.randint(5, 9)\n",
        "\n",
        "  print(f'======= TRY {i} | num_hiddens: {num_hiddens}, lr: {lr}, n_epochs: {n_epochs}, b_size = {batch_size} =======')\n",
        "\n",
        "  # 모델 세팅\n",
        "  model = TwoLayerNN(input_dim=784, num_hiddens=num_hiddens, num_classes=10)\n",
        "\n",
        "  # 모델 학습 및 Val 데이터로 평가\n",
        "  model.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)\n",
        "  print()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= TRY 0 | num_hiddens: 128, lr: 2.0568126419382463e-05, n_epochs: 10, b_size = 128 =======\n",
            "epoch 00 - train loss/acc: 2.751 0.105, valid loss/acc: 2.749 0.106\n",
            "epoch 01 - train loss/acc: 2.729 0.106, valid loss/acc: 2.727 0.106\n",
            "epoch 02 - train loss/acc: 2.709 0.106, valid loss/acc: 2.706 0.107\n",
            "epoch 03 - train loss/acc: 2.690 0.108, valid loss/acc: 2.687 0.108\n",
            "epoch 04 - train loss/acc: 2.672 0.109, valid loss/acc: 2.670 0.109\n",
            "epoch 05 - train loss/acc: 2.655 0.110, valid loss/acc: 2.653 0.110\n",
            "epoch 06 - train loss/acc: 2.640 0.113, valid loss/acc: 2.638 0.112\n",
            "epoch 07 - train loss/acc: 2.625 0.115, valid loss/acc: 2.623 0.114\n",
            "epoch 08 - train loss/acc: 2.611 0.117, valid loss/acc: 2.609 0.116\n",
            "epoch 09 - train loss/acc: 2.597 0.120, valid loss/acc: 2.595 0.117\n",
            "\n",
            "======= TRY 1 | num_hiddens: 64, lr: 3.584660864143891e-06, n_epochs: 10, b_size = 64 =======\n",
            "epoch 00 - train loss/acc: 2.882 0.113, valid loss/acc: 2.890 0.111\n",
            "epoch 01 - train loss/acc: 2.867 0.113, valid loss/acc: 2.875 0.111\n",
            "epoch 02 - train loss/acc: 2.853 0.113, valid loss/acc: 2.860 0.111\n",
            "epoch 03 - train loss/acc: 2.839 0.113, valid loss/acc: 2.846 0.111\n",
            "epoch 04 - train loss/acc: 2.825 0.113, valid loss/acc: 2.833 0.111\n",
            "epoch 05 - train loss/acc: 2.812 0.113, valid loss/acc: 2.819 0.111\n",
            "epoch 06 - train loss/acc: 2.799 0.113, valid loss/acc: 2.807 0.111\n",
            "epoch 07 - train loss/acc: 2.787 0.113, valid loss/acc: 2.794 0.111\n",
            "epoch 08 - train loss/acc: 2.775 0.113, valid loss/acc: 2.782 0.111\n",
            "epoch 09 - train loss/acc: 2.763 0.113, valid loss/acc: 2.770 0.111\n",
            "\n",
            "======= TRY 2 | num_hiddens: 128, lr: 1.0906162573403218e-05, n_epochs: 15, b_size = 32 =======\n",
            "epoch 00 - train loss/acc: 2.726 0.106, valid loss/acc: 2.724 0.106\n",
            "epoch 01 - train loss/acc: 2.685 0.108, valid loss/acc: 2.683 0.108\n",
            "epoch 02 - train loss/acc: 2.649 0.111, valid loss/acc: 2.647 0.111\n",
            "epoch 03 - train loss/acc: 2.618 0.116, valid loss/acc: 2.616 0.115\n",
            "epoch 04 - train loss/acc: 2.589 0.122, valid loss/acc: 2.588 0.120\n",
            "epoch 05 - train loss/acc: 2.564 0.126, valid loss/acc: 2.562 0.126\n",
            "epoch 06 - train loss/acc: 2.540 0.133, valid loss/acc: 2.538 0.132\n",
            "epoch 07 - train loss/acc: 2.518 0.139, valid loss/acc: 2.516 0.138\n",
            "epoch 08 - train loss/acc: 2.497 0.146, valid loss/acc: 2.496 0.144\n",
            "epoch 09 - train loss/acc: 2.478 0.152, valid loss/acc: 2.477 0.151\n",
            "epoch 10 - train loss/acc: 2.460 0.157, valid loss/acc: 2.459 0.156\n",
            "epoch 11 - train loss/acc: 2.443 0.162, valid loss/acc: 2.442 0.160\n",
            "epoch 12 - train loss/acc: 2.426 0.167, valid loss/acc: 2.425 0.166\n",
            "epoch 13 - train loss/acc: 2.411 0.173, valid loss/acc: 2.410 0.171\n",
            "epoch 14 - train loss/acc: 2.396 0.177, valid loss/acc: 2.395 0.176\n",
            "\n",
            "======= TRY 3 | num_hiddens: 32, lr: 1.7395725954727514e-06, n_epochs: 15, b_size = 64 =======\n",
            "epoch 00 - train loss/acc: 2.554 0.106, valid loss/acc: 2.550 0.106\n",
            "epoch 01 - train loss/acc: 2.553 0.106, valid loss/acc: 2.549 0.106\n",
            "epoch 02 - train loss/acc: 2.552 0.107, valid loss/acc: 2.548 0.106\n",
            "epoch 03 - train loss/acc: 2.551 0.107, valid loss/acc: 2.547 0.106\n",
            "epoch 04 - train loss/acc: 2.550 0.107, valid loss/acc: 2.546 0.106\n",
            "epoch 05 - train loss/acc: 2.549 0.107, valid loss/acc: 2.545 0.106\n",
            "epoch 06 - train loss/acc: 2.548 0.107, valid loss/acc: 2.544 0.106\n",
            "epoch 07 - train loss/acc: 2.547 0.107, valid loss/acc: 2.543 0.106\n",
            "epoch 08 - train loss/acc: 2.545 0.107, valid loss/acc: 2.542 0.107\n",
            "epoch 09 - train loss/acc: 2.544 0.107, valid loss/acc: 2.541 0.107\n",
            "epoch 10 - train loss/acc: 2.543 0.107, valid loss/acc: 2.540 0.107\n",
            "epoch 11 - train loss/acc: 2.542 0.107, valid loss/acc: 2.539 0.106\n",
            "epoch 12 - train loss/acc: 2.541 0.107, valid loss/acc: 2.538 0.106\n",
            "epoch 13 - train loss/acc: 2.540 0.108, valid loss/acc: 2.537 0.106\n",
            "epoch 14 - train loss/acc: 2.539 0.108, valid loss/acc: 2.536 0.107\n",
            "\n",
            "======= TRY 4 | num_hiddens: 32, lr: 4.8793127601548306e-06, n_epochs: 70, b_size = 32 =======\n",
            "epoch 00 - train loss/acc: 2.549 0.107, valid loss/acc: 2.545 0.106\n",
            "epoch 01 - train loss/acc: 2.543 0.107, valid loss/acc: 2.539 0.106\n",
            "epoch 02 - train loss/acc: 2.537 0.108, valid loss/acc: 2.534 0.107\n",
            "epoch 03 - train loss/acc: 2.532 0.109, valid loss/acc: 2.528 0.108\n",
            "epoch 04 - train loss/acc: 2.526 0.110, valid loss/acc: 2.523 0.109\n",
            "epoch 05 - train loss/acc: 2.521 0.110, valid loss/acc: 2.517 0.109\n",
            "epoch 06 - train loss/acc: 2.515 0.111, valid loss/acc: 2.512 0.110\n",
            "epoch 07 - train loss/acc: 2.510 0.111, valid loss/acc: 2.507 0.111\n",
            "epoch 08 - train loss/acc: 2.505 0.112, valid loss/acc: 2.502 0.112\n",
            "epoch 09 - train loss/acc: 2.500 0.113, valid loss/acc: 2.497 0.113\n",
            "epoch 10 - train loss/acc: 2.495 0.113, valid loss/acc: 2.492 0.113\n",
            "epoch 11 - train loss/acc: 2.490 0.113, valid loss/acc: 2.487 0.114\n",
            "epoch 12 - train loss/acc: 2.485 0.114, valid loss/acc: 2.483 0.115\n",
            "epoch 13 - train loss/acc: 2.481 0.114, valid loss/acc: 2.478 0.115\n",
            "epoch 14 - train loss/acc: 2.476 0.115, valid loss/acc: 2.473 0.115\n",
            "epoch 15 - train loss/acc: 2.471 0.115, valid loss/acc: 2.469 0.115\n",
            "epoch 16 - train loss/acc: 2.467 0.116, valid loss/acc: 2.465 0.116\n",
            "epoch 17 - train loss/acc: 2.462 0.116, valid loss/acc: 2.460 0.116\n",
            "epoch 18 - train loss/acc: 2.458 0.117, valid loss/acc: 2.456 0.116\n",
            "epoch 19 - train loss/acc: 2.454 0.118, valid loss/acc: 2.452 0.117\n",
            "epoch 20 - train loss/acc: 2.450 0.118, valid loss/acc: 2.448 0.118\n",
            "epoch 21 - train loss/acc: 2.445 0.119, valid loss/acc: 2.443 0.119\n",
            "epoch 22 - train loss/acc: 2.441 0.120, valid loss/acc: 2.439 0.119\n",
            "epoch 23 - train loss/acc: 2.437 0.120, valid loss/acc: 2.435 0.119\n",
            "epoch 24 - train loss/acc: 2.433 0.121, valid loss/acc: 2.431 0.119\n",
            "epoch 25 - train loss/acc: 2.429 0.122, valid loss/acc: 2.428 0.120\n",
            "epoch 26 - train loss/acc: 2.425 0.122, valid loss/acc: 2.424 0.121\n",
            "epoch 27 - train loss/acc: 2.422 0.123, valid loss/acc: 2.420 0.122\n",
            "epoch 28 - train loss/acc: 2.418 0.124, valid loss/acc: 2.416 0.123\n",
            "epoch 29 - train loss/acc: 2.414 0.124, valid loss/acc: 2.413 0.124\n",
            "epoch 30 - train loss/acc: 2.410 0.125, valid loss/acc: 2.409 0.125\n",
            "epoch 31 - train loss/acc: 2.407 0.125, valid loss/acc: 2.405 0.126\n",
            "epoch 32 - train loss/acc: 2.403 0.126, valid loss/acc: 2.402 0.126\n",
            "epoch 33 - train loss/acc: 2.400 0.127, valid loss/acc: 2.398 0.127\n",
            "epoch 34 - train loss/acc: 2.396 0.127, valid loss/acc: 2.395 0.128\n",
            "epoch 35 - train loss/acc: 2.393 0.128, valid loss/acc: 2.392 0.129\n",
            "epoch 36 - train loss/acc: 2.389 0.128, valid loss/acc: 2.388 0.129\n",
            "epoch 37 - train loss/acc: 2.386 0.129, valid loss/acc: 2.385 0.130\n",
            "epoch 38 - train loss/acc: 2.382 0.130, valid loss/acc: 2.382 0.130\n",
            "epoch 39 - train loss/acc: 2.379 0.130, valid loss/acc: 2.378 0.131\n",
            "epoch 40 - train loss/acc: 2.376 0.131, valid loss/acc: 2.375 0.132\n",
            "epoch 41 - train loss/acc: 2.373 0.132, valid loss/acc: 2.372 0.134\n",
            "epoch 42 - train loss/acc: 2.369 0.133, valid loss/acc: 2.369 0.134\n",
            "epoch 43 - train loss/acc: 2.366 0.133, valid loss/acc: 2.366 0.134\n",
            "epoch 44 - train loss/acc: 2.363 0.134, valid loss/acc: 2.363 0.135\n",
            "epoch 45 - train loss/acc: 2.360 0.135, valid loss/acc: 2.360 0.135\n",
            "epoch 46 - train loss/acc: 2.357 0.135, valid loss/acc: 2.357 0.136\n",
            "epoch 47 - train loss/acc: 2.354 0.136, valid loss/acc: 2.354 0.137\n",
            "epoch 48 - train loss/acc: 2.351 0.137, valid loss/acc: 2.351 0.138\n",
            "epoch 49 - train loss/acc: 2.348 0.138, valid loss/acc: 2.348 0.138\n",
            "epoch 50 - train loss/acc: 2.345 0.139, valid loss/acc: 2.345 0.139\n",
            "epoch 51 - train loss/acc: 2.342 0.139, valid loss/acc: 2.342 0.140\n",
            "epoch 52 - train loss/acc: 2.339 0.140, valid loss/acc: 2.339 0.139\n",
            "epoch 53 - train loss/acc: 2.336 0.141, valid loss/acc: 2.336 0.140\n",
            "epoch 54 - train loss/acc: 2.334 0.142, valid loss/acc: 2.334 0.141\n",
            "epoch 55 - train loss/acc: 2.331 0.142, valid loss/acc: 2.331 0.143\n",
            "epoch 56 - train loss/acc: 2.328 0.143, valid loss/acc: 2.328 0.143\n",
            "epoch 57 - train loss/acc: 2.325 0.144, valid loss/acc: 2.325 0.144\n",
            "epoch 58 - train loss/acc: 2.323 0.145, valid loss/acc: 2.323 0.145\n",
            "epoch 59 - train loss/acc: 2.320 0.145, valid loss/acc: 2.320 0.146\n",
            "epoch 60 - train loss/acc: 2.317 0.146, valid loss/acc: 2.318 0.146\n",
            "epoch 61 - train loss/acc: 2.315 0.147, valid loss/acc: 2.315 0.148\n",
            "epoch 62 - train loss/acc: 2.312 0.148, valid loss/acc: 2.312 0.149\n",
            "epoch 63 - train loss/acc: 2.309 0.148, valid loss/acc: 2.310 0.150\n",
            "epoch 64 - train loss/acc: 2.307 0.149, valid loss/acc: 2.307 0.151\n",
            "epoch 65 - train loss/acc: 2.304 0.150, valid loss/acc: 2.305 0.152\n",
            "epoch 66 - train loss/acc: 2.302 0.151, valid loss/acc: 2.302 0.153\n",
            "epoch 67 - train loss/acc: 2.299 0.152, valid loss/acc: 2.300 0.153\n",
            "epoch 68 - train loss/acc: 2.297 0.153, valid loss/acc: 2.297 0.153\n",
            "epoch 69 - train loss/acc: 2.294 0.154, valid loss/acc: 2.295 0.155\n",
            "\n",
            "======= TRY 5 | num_hiddens: 64, lr: 2.187912573208277e-05, n_epochs: 55, b_size = 256 =======\n",
            "epoch 00 - train loss/acc: 2.874 0.113, valid loss/acc: 2.882 0.111\n",
            "epoch 01 - train loss/acc: 2.852 0.113, valid loss/acc: 2.860 0.111\n",
            "epoch 02 - train loss/acc: 2.831 0.113, valid loss/acc: 2.839 0.111\n",
            "epoch 03 - train loss/acc: 2.811 0.113, valid loss/acc: 2.818 0.111\n",
            "epoch 04 - train loss/acc: 2.791 0.113, valid loss/acc: 2.799 0.111\n",
            "epoch 05 - train loss/acc: 2.773 0.113, valid loss/acc: 2.780 0.111\n",
            "epoch 06 - train loss/acc: 2.755 0.113, valid loss/acc: 2.763 0.111\n",
            "epoch 07 - train loss/acc: 2.738 0.113, valid loss/acc: 2.746 0.111\n",
            "epoch 08 - train loss/acc: 2.722 0.113, valid loss/acc: 2.730 0.111\n",
            "epoch 09 - train loss/acc: 2.707 0.113, valid loss/acc: 2.714 0.111\n",
            "epoch 10 - train loss/acc: 2.692 0.113, valid loss/acc: 2.699 0.111\n",
            "epoch 11 - train loss/acc: 2.677 0.113, valid loss/acc: 2.685 0.111\n",
            "epoch 12 - train loss/acc: 2.664 0.113, valid loss/acc: 2.671 0.111\n",
            "epoch 13 - train loss/acc: 2.650 0.113, valid loss/acc: 2.658 0.112\n",
            "epoch 14 - train loss/acc: 2.638 0.113, valid loss/acc: 2.645 0.112\n",
            "epoch 15 - train loss/acc: 2.625 0.113, valid loss/acc: 2.633 0.112\n",
            "epoch 16 - train loss/acc: 2.614 0.113, valid loss/acc: 2.621 0.112\n",
            "epoch 17 - train loss/acc: 2.602 0.113, valid loss/acc: 2.609 0.112\n",
            "epoch 18 - train loss/acc: 2.591 0.113, valid loss/acc: 2.599 0.112\n",
            "epoch 19 - train loss/acc: 2.581 0.113, valid loss/acc: 2.588 0.112\n",
            "epoch 20 - train loss/acc: 2.571 0.114, valid loss/acc: 2.578 0.112\n",
            "epoch 21 - train loss/acc: 2.561 0.114, valid loss/acc: 2.568 0.112\n",
            "epoch 22 - train loss/acc: 2.552 0.114, valid loss/acc: 2.558 0.113\n",
            "epoch 23 - train loss/acc: 2.542 0.114, valid loss/acc: 2.549 0.113\n",
            "epoch 24 - train loss/acc: 2.534 0.115, valid loss/acc: 2.540 0.113\n",
            "epoch 25 - train loss/acc: 2.525 0.115, valid loss/acc: 2.532 0.114\n",
            "epoch 26 - train loss/acc: 2.517 0.116, valid loss/acc: 2.523 0.114\n",
            "epoch 27 - train loss/acc: 2.509 0.116, valid loss/acc: 2.515 0.114\n",
            "epoch 28 - train loss/acc: 2.501 0.117, valid loss/acc: 2.508 0.115\n",
            "epoch 29 - train loss/acc: 2.493 0.117, valid loss/acc: 2.500 0.116\n",
            "epoch 30 - train loss/acc: 2.486 0.117, valid loss/acc: 2.493 0.116\n",
            "epoch 31 - train loss/acc: 2.479 0.118, valid loss/acc: 2.486 0.117\n",
            "epoch 32 - train loss/acc: 2.472 0.118, valid loss/acc: 2.479 0.117\n",
            "epoch 33 - train loss/acc: 2.465 0.119, valid loss/acc: 2.472 0.118\n",
            "epoch 34 - train loss/acc: 2.459 0.120, valid loss/acc: 2.465 0.119\n",
            "epoch 35 - train loss/acc: 2.452 0.120, valid loss/acc: 2.459 0.120\n",
            "epoch 36 - train loss/acc: 2.446 0.121, valid loss/acc: 2.453 0.120\n",
            "epoch 37 - train loss/acc: 2.440 0.122, valid loss/acc: 2.447 0.121\n",
            "epoch 38 - train loss/acc: 2.434 0.123, valid loss/acc: 2.441 0.122\n",
            "epoch 39 - train loss/acc: 2.429 0.124, valid loss/acc: 2.435 0.123\n",
            "epoch 40 - train loss/acc: 2.423 0.125, valid loss/acc: 2.430 0.123\n",
            "epoch 41 - train loss/acc: 2.418 0.126, valid loss/acc: 2.424 0.124\n",
            "epoch 42 - train loss/acc: 2.412 0.126, valid loss/acc: 2.419 0.124\n",
            "epoch 43 - train loss/acc: 2.407 0.127, valid loss/acc: 2.414 0.125\n",
            "epoch 44 - train loss/acc: 2.402 0.128, valid loss/acc: 2.409 0.126\n",
            "epoch 45 - train loss/acc: 2.397 0.129, valid loss/acc: 2.404 0.127\n",
            "epoch 46 - train loss/acc: 2.393 0.130, valid loss/acc: 2.399 0.128\n",
            "epoch 47 - train loss/acc: 2.388 0.131, valid loss/acc: 2.394 0.129\n",
            "epoch 48 - train loss/acc: 2.383 0.132, valid loss/acc: 2.390 0.130\n",
            "epoch 49 - train loss/acc: 2.379 0.132, valid loss/acc: 2.385 0.130\n",
            "epoch 50 - train loss/acc: 2.374 0.133, valid loss/acc: 2.381 0.132\n",
            "epoch 51 - train loss/acc: 2.370 0.134, valid loss/acc: 2.377 0.132\n",
            "epoch 52 - train loss/acc: 2.366 0.135, valid loss/acc: 2.373 0.133\n",
            "epoch 53 - train loss/acc: 2.362 0.136, valid loss/acc: 2.368 0.134\n",
            "epoch 54 - train loss/acc: 2.358 0.137, valid loss/acc: 2.364 0.135\n",
            "\n",
            "======= TRY 6 | num_hiddens: 128, lr: 3.246747330056898e-05, n_epochs: 50, b_size = 128 =======\n",
            "epoch 00 - train loss/acc: 2.738 0.105, valid loss/acc: 2.736 0.106\n",
            "epoch 01 - train loss/acc: 2.705 0.107, valid loss/acc: 2.703 0.107\n",
            "epoch 02 - train loss/acc: 2.676 0.108, valid loss/acc: 2.674 0.109\n",
            "epoch 03 - train loss/acc: 2.650 0.111, valid loss/acc: 2.648 0.111\n",
            "epoch 04 - train loss/acc: 2.626 0.115, valid loss/acc: 2.624 0.114\n",
            "epoch 05 - train loss/acc: 2.604 0.118, valid loss/acc: 2.602 0.117\n",
            "epoch 06 - train loss/acc: 2.584 0.123, valid loss/acc: 2.582 0.121\n",
            "epoch 07 - train loss/acc: 2.565 0.126, valid loss/acc: 2.563 0.125\n",
            "epoch 08 - train loss/acc: 2.547 0.132, valid loss/acc: 2.545 0.130\n",
            "epoch 09 - train loss/acc: 2.530 0.136, valid loss/acc: 2.528 0.135\n",
            "epoch 10 - train loss/acc: 2.514 0.140, valid loss/acc: 2.512 0.140\n",
            "epoch 11 - train loss/acc: 2.499 0.145, valid loss/acc: 2.497 0.144\n",
            "epoch 12 - train loss/acc: 2.484 0.150, valid loss/acc: 2.483 0.148\n",
            "epoch 13 - train loss/acc: 2.470 0.154, valid loss/acc: 2.469 0.153\n",
            "epoch 14 - train loss/acc: 2.457 0.158, valid loss/acc: 2.456 0.156\n",
            "epoch 15 - train loss/acc: 2.444 0.161, valid loss/acc: 2.443 0.159\n",
            "epoch 16 - train loss/acc: 2.432 0.166, valid loss/acc: 2.431 0.164\n",
            "epoch 17 - train loss/acc: 2.420 0.170, valid loss/acc: 2.419 0.168\n",
            "epoch 18 - train loss/acc: 2.409 0.173, valid loss/acc: 2.408 0.172\n",
            "epoch 19 - train loss/acc: 2.398 0.177, valid loss/acc: 2.397 0.175\n",
            "epoch 20 - train loss/acc: 2.387 0.180, valid loss/acc: 2.386 0.179\n",
            "epoch 21 - train loss/acc: 2.377 0.183, valid loss/acc: 2.376 0.182\n",
            "epoch 22 - train loss/acc: 2.367 0.186, valid loss/acc: 2.366 0.186\n",
            "epoch 23 - train loss/acc: 2.358 0.190, valid loss/acc: 2.357 0.188\n",
            "epoch 24 - train loss/acc: 2.348 0.194, valid loss/acc: 2.348 0.191\n",
            "epoch 25 - train loss/acc: 2.339 0.197, valid loss/acc: 2.339 0.194\n",
            "epoch 26 - train loss/acc: 2.331 0.200, valid loss/acc: 2.330 0.198\n",
            "epoch 27 - train loss/acc: 2.322 0.203, valid loss/acc: 2.322 0.201\n",
            "epoch 28 - train loss/acc: 2.314 0.207, valid loss/acc: 2.314 0.204\n",
            "epoch 29 - train loss/acc: 2.306 0.211, valid loss/acc: 2.306 0.207\n",
            "epoch 30 - train loss/acc: 2.299 0.214, valid loss/acc: 2.298 0.210\n",
            "epoch 31 - train loss/acc: 2.291 0.218, valid loss/acc: 2.291 0.213\n",
            "epoch 32 - train loss/acc: 2.284 0.222, valid loss/acc: 2.284 0.217\n",
            "epoch 33 - train loss/acc: 2.277 0.225, valid loss/acc: 2.277 0.220\n",
            "epoch 34 - train loss/acc: 2.271 0.228, valid loss/acc: 2.271 0.224\n",
            "epoch 35 - train loss/acc: 2.264 0.232, valid loss/acc: 2.264 0.227\n",
            "epoch 36 - train loss/acc: 2.258 0.235, valid loss/acc: 2.258 0.231\n",
            "epoch 37 - train loss/acc: 2.252 0.238, valid loss/acc: 2.252 0.235\n",
            "epoch 38 - train loss/acc: 2.246 0.241, valid loss/acc: 2.246 0.238\n",
            "epoch 39 - train loss/acc: 2.240 0.244, valid loss/acc: 2.240 0.241\n",
            "epoch 40 - train loss/acc: 2.234 0.248, valid loss/acc: 2.234 0.244\n",
            "epoch 41 - train loss/acc: 2.229 0.250, valid loss/acc: 2.229 0.247\n",
            "epoch 42 - train loss/acc: 2.223 0.254, valid loss/acc: 2.224 0.251\n",
            "epoch 43 - train loss/acc: 2.218 0.257, valid loss/acc: 2.218 0.254\n",
            "epoch 44 - train loss/acc: 2.213 0.260, valid loss/acc: 2.213 0.258\n",
            "epoch 45 - train loss/acc: 2.208 0.263, valid loss/acc: 2.208 0.260\n",
            "epoch 46 - train loss/acc: 2.203 0.266, valid loss/acc: 2.204 0.263\n",
            "epoch 47 - train loss/acc: 2.198 0.270, valid loss/acc: 2.199 0.265\n",
            "epoch 48 - train loss/acc: 2.194 0.273, valid loss/acc: 2.194 0.269\n",
            "epoch 49 - train loss/acc: 2.189 0.276, valid loss/acc: 2.190 0.272\n",
            "\n",
            "======= TRY 7 | num_hiddens: 32, lr: 9.277175212354456e-06, n_epochs: 85, b_size = 32 =======\n",
            "epoch 00 - train loss/acc: 2.544 0.107, valid loss/acc: 2.540 0.107\n",
            "epoch 01 - train loss/acc: 2.533 0.109, valid loss/acc: 2.529 0.107\n",
            "epoch 02 - train loss/acc: 2.522 0.110, valid loss/acc: 2.519 0.109\n",
            "epoch 03 - train loss/acc: 2.512 0.111, valid loss/acc: 2.509 0.111\n",
            "epoch 04 - train loss/acc: 2.502 0.112, valid loss/acc: 2.499 0.113\n",
            "epoch 05 - train loss/acc: 2.493 0.113, valid loss/acc: 2.490 0.113\n",
            "epoch 06 - train loss/acc: 2.484 0.114, valid loss/acc: 2.481 0.115\n",
            "epoch 07 - train loss/acc: 2.475 0.115, valid loss/acc: 2.472 0.115\n",
            "epoch 08 - train loss/acc: 2.466 0.116, valid loss/acc: 2.464 0.116\n",
            "epoch 09 - train loss/acc: 2.458 0.117, valid loss/acc: 2.456 0.116\n",
            "epoch 10 - train loss/acc: 2.450 0.118, valid loss/acc: 2.448 0.118\n",
            "epoch 11 - train loss/acc: 2.442 0.120, valid loss/acc: 2.440 0.119\n",
            "epoch 12 - train loss/acc: 2.434 0.121, valid loss/acc: 2.433 0.119\n",
            "epoch 13 - train loss/acc: 2.427 0.122, valid loss/acc: 2.425 0.121\n",
            "epoch 14 - train loss/acc: 2.420 0.123, valid loss/acc: 2.418 0.122\n",
            "epoch 15 - train loss/acc: 2.412 0.124, valid loss/acc: 2.411 0.124\n",
            "epoch 16 - train loss/acc: 2.406 0.126, valid loss/acc: 2.404 0.126\n",
            "epoch 17 - train loss/acc: 2.399 0.127, valid loss/acc: 2.398 0.127\n",
            "epoch 18 - train loss/acc: 2.392 0.128, valid loss/acc: 2.391 0.129\n",
            "epoch 19 - train loss/acc: 2.386 0.129, valid loss/acc: 2.385 0.130\n",
            "epoch 20 - train loss/acc: 2.379 0.130, valid loss/acc: 2.379 0.131\n",
            "epoch 21 - train loss/acc: 2.373 0.132, valid loss/acc: 2.372 0.133\n",
            "epoch 22 - train loss/acc: 2.367 0.133, valid loss/acc: 2.367 0.134\n",
            "epoch 23 - train loss/acc: 2.361 0.134, valid loss/acc: 2.361 0.135\n",
            "epoch 24 - train loss/acc: 2.355 0.136, valid loss/acc: 2.355 0.136\n",
            "epoch 25 - train loss/acc: 2.350 0.137, valid loss/acc: 2.349 0.138\n",
            "epoch 26 - train loss/acc: 2.344 0.139, valid loss/acc: 2.344 0.139\n",
            "epoch 27 - train loss/acc: 2.339 0.140, valid loss/acc: 2.339 0.140\n",
            "epoch 28 - train loss/acc: 2.333 0.142, valid loss/acc: 2.333 0.142\n",
            "epoch 29 - train loss/acc: 2.328 0.143, valid loss/acc: 2.328 0.143\n",
            "epoch 30 - train loss/acc: 2.323 0.145, valid loss/acc: 2.323 0.144\n",
            "epoch 31 - train loss/acc: 2.318 0.146, valid loss/acc: 2.318 0.146\n",
            "epoch 32 - train loss/acc: 2.313 0.147, valid loss/acc: 2.313 0.149\n",
            "epoch 33 - train loss/acc: 2.308 0.149, valid loss/acc: 2.308 0.150\n",
            "epoch 34 - train loss/acc: 2.303 0.151, valid loss/acc: 2.303 0.152\n",
            "epoch 35 - train loss/acc: 2.298 0.152, valid loss/acc: 2.299 0.153\n",
            "epoch 36 - train loss/acc: 2.293 0.154, valid loss/acc: 2.294 0.155\n",
            "epoch 37 - train loss/acc: 2.289 0.155, valid loss/acc: 2.290 0.157\n",
            "epoch 38 - train loss/acc: 2.284 0.157, valid loss/acc: 2.285 0.158\n",
            "epoch 39 - train loss/acc: 2.280 0.158, valid loss/acc: 2.281 0.159\n",
            "epoch 40 - train loss/acc: 2.275 0.160, valid loss/acc: 2.276 0.160\n",
            "epoch 41 - train loss/acc: 2.271 0.162, valid loss/acc: 2.272 0.162\n",
            "epoch 42 - train loss/acc: 2.267 0.163, valid loss/acc: 2.268 0.163\n",
            "epoch 43 - train loss/acc: 2.262 0.165, valid loss/acc: 2.264 0.165\n",
            "epoch 44 - train loss/acc: 2.258 0.166, valid loss/acc: 2.260 0.166\n",
            "epoch 45 - train loss/acc: 2.254 0.168, valid loss/acc: 2.256 0.168\n",
            "epoch 46 - train loss/acc: 2.250 0.170, valid loss/acc: 2.252 0.169\n",
            "epoch 47 - train loss/acc: 2.246 0.172, valid loss/acc: 2.248 0.171\n",
            "epoch 48 - train loss/acc: 2.242 0.174, valid loss/acc: 2.244 0.173\n",
            "epoch 49 - train loss/acc: 2.238 0.175, valid loss/acc: 2.240 0.174\n",
            "epoch 50 - train loss/acc: 2.234 0.177, valid loss/acc: 2.236 0.176\n",
            "epoch 51 - train loss/acc: 2.231 0.178, valid loss/acc: 2.232 0.177\n",
            "epoch 52 - train loss/acc: 2.227 0.180, valid loss/acc: 2.229 0.178\n",
            "epoch 53 - train loss/acc: 2.223 0.182, valid loss/acc: 2.225 0.180\n",
            "epoch 54 - train loss/acc: 2.219 0.184, valid loss/acc: 2.221 0.182\n",
            "epoch 55 - train loss/acc: 2.216 0.185, valid loss/acc: 2.218 0.184\n",
            "epoch 56 - train loss/acc: 2.212 0.187, valid loss/acc: 2.214 0.187\n",
            "epoch 57 - train loss/acc: 2.209 0.189, valid loss/acc: 2.211 0.189\n",
            "epoch 58 - train loss/acc: 2.205 0.191, valid loss/acc: 2.207 0.191\n",
            "epoch 59 - train loss/acc: 2.201 0.193, valid loss/acc: 2.204 0.193\n",
            "epoch 60 - train loss/acc: 2.198 0.195, valid loss/acc: 2.200 0.195\n",
            "epoch 61 - train loss/acc: 2.195 0.196, valid loss/acc: 2.197 0.197\n",
            "epoch 62 - train loss/acc: 2.191 0.199, valid loss/acc: 2.194 0.199\n",
            "epoch 63 - train loss/acc: 2.188 0.201, valid loss/acc: 2.190 0.200\n",
            "epoch 64 - train loss/acc: 2.185 0.203, valid loss/acc: 2.187 0.201\n",
            "epoch 65 - train loss/acc: 2.181 0.205, valid loss/acc: 2.184 0.203\n",
            "epoch 66 - train loss/acc: 2.178 0.206, valid loss/acc: 2.181 0.207\n",
            "epoch 67 - train loss/acc: 2.175 0.209, valid loss/acc: 2.177 0.209\n",
            "epoch 68 - train loss/acc: 2.171 0.211, valid loss/acc: 2.174 0.211\n",
            "epoch 69 - train loss/acc: 2.168 0.213, valid loss/acc: 2.171 0.213\n",
            "epoch 70 - train loss/acc: 2.165 0.215, valid loss/acc: 2.168 0.215\n",
            "epoch 71 - train loss/acc: 2.162 0.217, valid loss/acc: 2.165 0.217\n",
            "epoch 72 - train loss/acc: 2.159 0.220, valid loss/acc: 2.162 0.219\n",
            "epoch 73 - train loss/acc: 2.156 0.222, valid loss/acc: 2.159 0.221\n",
            "epoch 74 - train loss/acc: 2.153 0.224, valid loss/acc: 2.156 0.223\n",
            "epoch 75 - train loss/acc: 2.150 0.227, valid loss/acc: 2.153 0.225\n",
            "epoch 76 - train loss/acc: 2.147 0.229, valid loss/acc: 2.150 0.228\n",
            "epoch 77 - train loss/acc: 2.144 0.232, valid loss/acc: 2.147 0.231\n",
            "epoch 78 - train loss/acc: 2.141 0.234, valid loss/acc: 2.144 0.234\n",
            "epoch 79 - train loss/acc: 2.138 0.237, valid loss/acc: 2.141 0.236\n",
            "epoch 80 - train loss/acc: 2.135 0.240, valid loss/acc: 2.138 0.239\n",
            "epoch 81 - train loss/acc: 2.132 0.243, valid loss/acc: 2.135 0.242\n",
            "epoch 82 - train loss/acc: 2.129 0.246, valid loss/acc: 2.132 0.244\n",
            "epoch 83 - train loss/acc: 2.126 0.248, valid loss/acc: 2.129 0.247\n",
            "epoch 84 - train loss/acc: 2.123 0.252, valid loss/acc: 2.126 0.249\n",
            "\n",
            "======= TRY 8 | num_hiddens: 256, lr: 1.25982833423243e-06, n_epochs: 75, b_size = 32 =======\n",
            "epoch 00 - train loss/acc: 2.906 0.137, valid loss/acc: 2.905 0.135\n",
            "epoch 01 - train loss/acc: 2.894 0.137, valid loss/acc: 2.893 0.135\n",
            "epoch 02 - train loss/acc: 2.882 0.137, valid loss/acc: 2.881 0.136\n",
            "epoch 03 - train loss/acc: 2.870 0.138, valid loss/acc: 2.870 0.136\n",
            "epoch 04 - train loss/acc: 2.859 0.138, valid loss/acc: 2.859 0.136\n",
            "epoch 05 - train loss/acc: 2.848 0.138, valid loss/acc: 2.848 0.137\n",
            "epoch 06 - train loss/acc: 2.838 0.138, valid loss/acc: 2.837 0.137\n",
            "epoch 07 - train loss/acc: 2.827 0.138, valid loss/acc: 2.827 0.137\n",
            "epoch 08 - train loss/acc: 2.817 0.138, valid loss/acc: 2.816 0.137\n",
            "epoch 09 - train loss/acc: 2.807 0.138, valid loss/acc: 2.806 0.137\n",
            "epoch 10 - train loss/acc: 2.797 0.138, valid loss/acc: 2.797 0.138\n",
            "epoch 11 - train loss/acc: 2.787 0.138, valid loss/acc: 2.787 0.138\n",
            "epoch 12 - train loss/acc: 2.778 0.138, valid loss/acc: 2.778 0.138\n",
            "epoch 13 - train loss/acc: 2.769 0.138, valid loss/acc: 2.768 0.139\n",
            "epoch 14 - train loss/acc: 2.760 0.138, valid loss/acc: 2.760 0.139\n",
            "epoch 15 - train loss/acc: 2.751 0.138, valid loss/acc: 2.751 0.139\n",
            "epoch 16 - train loss/acc: 2.742 0.138, valid loss/acc: 2.742 0.139\n",
            "epoch 17 - train loss/acc: 2.734 0.138, valid loss/acc: 2.734 0.138\n",
            "epoch 18 - train loss/acc: 2.725 0.137, valid loss/acc: 2.725 0.138\n",
            "epoch 19 - train loss/acc: 2.717 0.137, valid loss/acc: 2.717 0.138\n",
            "epoch 20 - train loss/acc: 2.709 0.137, valid loss/acc: 2.709 0.138\n",
            "epoch 21 - train loss/acc: 2.702 0.137, valid loss/acc: 2.702 0.137\n",
            "epoch 22 - train loss/acc: 2.694 0.137, valid loss/acc: 2.694 0.137\n",
            "epoch 23 - train loss/acc: 2.686 0.137, valid loss/acc: 2.687 0.136\n",
            "epoch 24 - train loss/acc: 2.679 0.137, valid loss/acc: 2.679 0.136\n",
            "epoch 25 - train loss/acc: 2.672 0.137, valid loss/acc: 2.672 0.136\n",
            "epoch 26 - train loss/acc: 2.665 0.137, valid loss/acc: 2.665 0.136\n",
            "epoch 27 - train loss/acc: 2.658 0.137, valid loss/acc: 2.658 0.136\n",
            "epoch 28 - train loss/acc: 2.651 0.137, valid loss/acc: 2.651 0.136\n",
            "epoch 29 - train loss/acc: 2.644 0.137, valid loss/acc: 2.645 0.136\n",
            "epoch 30 - train loss/acc: 2.638 0.137, valid loss/acc: 2.638 0.136\n",
            "epoch 31 - train loss/acc: 2.631 0.136, valid loss/acc: 2.632 0.136\n",
            "epoch 32 - train loss/acc: 2.625 0.136, valid loss/acc: 2.626 0.136\n",
            "epoch 33 - train loss/acc: 2.619 0.136, valid loss/acc: 2.619 0.136\n",
            "epoch 34 - train loss/acc: 2.613 0.136, valid loss/acc: 2.613 0.136\n",
            "epoch 35 - train loss/acc: 2.607 0.136, valid loss/acc: 2.607 0.136\n",
            "epoch 36 - train loss/acc: 2.601 0.136, valid loss/acc: 2.602 0.136\n",
            "epoch 37 - train loss/acc: 2.595 0.136, valid loss/acc: 2.596 0.136\n",
            "epoch 38 - train loss/acc: 2.589 0.136, valid loss/acc: 2.590 0.136\n",
            "epoch 39 - train loss/acc: 2.584 0.136, valid loss/acc: 2.585 0.135\n",
            "epoch 40 - train loss/acc: 2.578 0.136, valid loss/acc: 2.579 0.135\n",
            "epoch 41 - train loss/acc: 2.573 0.136, valid loss/acc: 2.574 0.135\n",
            "epoch 42 - train loss/acc: 2.567 0.136, valid loss/acc: 2.569 0.136\n",
            "epoch 43 - train loss/acc: 2.562 0.136, valid loss/acc: 2.563 0.136\n",
            "epoch 44 - train loss/acc: 2.557 0.136, valid loss/acc: 2.558 0.136\n",
            "epoch 45 - train loss/acc: 2.552 0.136, valid loss/acc: 2.553 0.136\n",
            "epoch 46 - train loss/acc: 2.547 0.136, valid loss/acc: 2.548 0.136\n",
            "epoch 47 - train loss/acc: 2.542 0.137, valid loss/acc: 2.544 0.137\n",
            "epoch 48 - train loss/acc: 2.537 0.137, valid loss/acc: 2.539 0.137\n",
            "epoch 49 - train loss/acc: 2.533 0.138, valid loss/acc: 2.534 0.137\n",
            "epoch 50 - train loss/acc: 2.528 0.138, valid loss/acc: 2.530 0.137\n",
            "epoch 51 - train loss/acc: 2.523 0.138, valid loss/acc: 2.525 0.137\n",
            "epoch 52 - train loss/acc: 2.519 0.139, valid loss/acc: 2.521 0.138\n",
            "epoch 53 - train loss/acc: 2.515 0.139, valid loss/acc: 2.516 0.138\n",
            "epoch 54 - train loss/acc: 2.510 0.140, valid loss/acc: 2.512 0.139\n",
            "epoch 55 - train loss/acc: 2.506 0.140, valid loss/acc: 2.508 0.139\n",
            "epoch 56 - train loss/acc: 2.502 0.141, valid loss/acc: 2.503 0.140\n",
            "epoch 57 - train loss/acc: 2.498 0.141, valid loss/acc: 2.499 0.140\n",
            "epoch 58 - train loss/acc: 2.493 0.142, valid loss/acc: 2.495 0.140\n",
            "epoch 59 - train loss/acc: 2.489 0.142, valid loss/acc: 2.491 0.141\n",
            "epoch 60 - train loss/acc: 2.485 0.142, valid loss/acc: 2.487 0.142\n",
            "epoch 61 - train loss/acc: 2.481 0.143, valid loss/acc: 2.483 0.142\n",
            "epoch 62 - train loss/acc: 2.478 0.144, valid loss/acc: 2.480 0.142\n",
            "epoch 63 - train loss/acc: 2.474 0.144, valid loss/acc: 2.476 0.143\n",
            "epoch 64 - train loss/acc: 2.470 0.145, valid loss/acc: 2.472 0.143\n",
            "epoch 65 - train loss/acc: 2.466 0.145, valid loss/acc: 2.468 0.144\n",
            "epoch 66 - train loss/acc: 2.463 0.146, valid loss/acc: 2.465 0.145\n",
            "epoch 67 - train loss/acc: 2.459 0.147, valid loss/acc: 2.461 0.146\n",
            "epoch 68 - train loss/acc: 2.456 0.147, valid loss/acc: 2.458 0.146\n",
            "epoch 69 - train loss/acc: 2.452 0.148, valid loss/acc: 2.454 0.146\n",
            "epoch 70 - train loss/acc: 2.449 0.148, valid loss/acc: 2.451 0.145\n",
            "epoch 71 - train loss/acc: 2.445 0.149, valid loss/acc: 2.447 0.146\n",
            "epoch 72 - train loss/acc: 2.442 0.149, valid loss/acc: 2.444 0.147\n",
            "epoch 73 - train loss/acc: 2.438 0.150, valid loss/acc: 2.441 0.148\n",
            "epoch 74 - train loss/acc: 2.435 0.151, valid loss/acc: 2.438 0.149\n",
            "\n",
            "======= TRY 9 | num_hiddens: 256, lr: 3.938169605889904e-06, n_epochs: 35, b_size = 32 =======\n",
            "epoch 00 - train loss/acc: 2.880 0.137, valid loss/acc: 2.880 0.136\n",
            "epoch 01 - train loss/acc: 2.846 0.138, valid loss/acc: 2.845 0.137\n",
            "epoch 02 - train loss/acc: 2.813 0.138, valid loss/acc: 2.813 0.137\n",
            "epoch 03 - train loss/acc: 2.783 0.138, valid loss/acc: 2.782 0.138\n",
            "epoch 04 - train loss/acc: 2.754 0.138, valid loss/acc: 2.754 0.139\n",
            "epoch 05 - train loss/acc: 2.727 0.137, valid loss/acc: 2.727 0.138\n",
            "epoch 06 - train loss/acc: 2.702 0.137, valid loss/acc: 2.703 0.137\n",
            "epoch 07 - train loss/acc: 2.679 0.137, valid loss/acc: 2.679 0.136\n",
            "epoch 08 - train loss/acc: 2.657 0.137, valid loss/acc: 2.657 0.136\n",
            "epoch 09 - train loss/acc: 2.636 0.137, valid loss/acc: 2.637 0.136\n",
            "epoch 10 - train loss/acc: 2.616 0.136, valid loss/acc: 2.617 0.136\n",
            "epoch 11 - train loss/acc: 2.598 0.136, valid loss/acc: 2.599 0.136\n",
            "epoch 12 - train loss/acc: 2.580 0.136, valid loss/acc: 2.581 0.135\n",
            "epoch 13 - train loss/acc: 2.563 0.136, valid loss/acc: 2.565 0.136\n",
            "epoch 14 - train loss/acc: 2.548 0.136, valid loss/acc: 2.549 0.136\n",
            "epoch 15 - train loss/acc: 2.533 0.138, valid loss/acc: 2.534 0.137\n",
            "epoch 16 - train loss/acc: 2.518 0.139, valid loss/acc: 2.520 0.138\n",
            "epoch 17 - train loss/acc: 2.505 0.140, valid loss/acc: 2.506 0.139\n",
            "epoch 18 - train loss/acc: 2.492 0.142, valid loss/acc: 2.494 0.140\n",
            "epoch 19 - train loss/acc: 2.479 0.143, valid loss/acc: 2.481 0.142\n",
            "epoch 20 - train loss/acc: 2.468 0.145, valid loss/acc: 2.470 0.144\n",
            "epoch 21 - train loss/acc: 2.456 0.147, valid loss/acc: 2.458 0.145\n",
            "epoch 22 - train loss/acc: 2.445 0.149, valid loss/acc: 2.448 0.146\n",
            "epoch 23 - train loss/acc: 2.435 0.151, valid loss/acc: 2.437 0.149\n",
            "epoch 24 - train loss/acc: 2.425 0.152, valid loss/acc: 2.428 0.151\n",
            "epoch 25 - train loss/acc: 2.416 0.154, valid loss/acc: 2.418 0.153\n",
            "epoch 26 - train loss/acc: 2.406 0.156, valid loss/acc: 2.409 0.155\n",
            "epoch 27 - train loss/acc: 2.397 0.158, valid loss/acc: 2.400 0.157\n",
            "epoch 28 - train loss/acc: 2.389 0.161, valid loss/acc: 2.392 0.159\n",
            "epoch 29 - train loss/acc: 2.381 0.164, valid loss/acc: 2.384 0.161\n",
            "epoch 30 - train loss/acc: 2.373 0.167, valid loss/acc: 2.376 0.164\n",
            "epoch 31 - train loss/acc: 2.365 0.170, valid loss/acc: 2.368 0.167\n",
            "epoch 32 - train loss/acc: 2.358 0.175, valid loss/acc: 2.361 0.172\n",
            "epoch 33 - train loss/acc: 2.351 0.179, valid loss/acc: 2.354 0.175\n",
            "epoch 34 - train loss/acc: 2.344 0.183, valid loss/acc: 2.347 0.179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "실험결과, TRY6 또는 TRY7 의 조합이 가장 우수한 성능을 보였다. \\\n",
        "따라서 아래 최종 테스트는 TRY6 의 세팅으로 수행하였다."
      ],
      "metadata": {
        "id": "ap1xZ09phQHm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTCqVT4S0Tm5"
      },
      "source": [
        "# model instantiation\n",
        "model = TwoLayerNN(input_dim=784, num_hiddens=128, num_classes=10) # 데이터셋에 맞게 값 주입"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cWb6xg0NxOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c7f86fb-eecc-4c4d-81b0-774b410942fe"
      },
      "source": [
        "# train the model & evaluate with validation data\n",
        "lr, n_epochs, batch_size = 3.246747330056898e-05, 50, 128\n",
        "model.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 00 - train loss/acc: 2.738 0.105, valid loss/acc: 2.736 0.106\n",
            "epoch 01 - train loss/acc: 2.705 0.107, valid loss/acc: 2.703 0.107\n",
            "epoch 02 - train loss/acc: 2.676 0.108, valid loss/acc: 2.674 0.109\n",
            "epoch 03 - train loss/acc: 2.650 0.111, valid loss/acc: 2.648 0.111\n",
            "epoch 04 - train loss/acc: 2.626 0.115, valid loss/acc: 2.624 0.114\n",
            "epoch 05 - train loss/acc: 2.604 0.118, valid loss/acc: 2.602 0.117\n",
            "epoch 06 - train loss/acc: 2.584 0.123, valid loss/acc: 2.582 0.121\n",
            "epoch 07 - train loss/acc: 2.565 0.126, valid loss/acc: 2.563 0.125\n",
            "epoch 08 - train loss/acc: 2.547 0.132, valid loss/acc: 2.545 0.130\n",
            "epoch 09 - train loss/acc: 2.530 0.136, valid loss/acc: 2.528 0.135\n",
            "epoch 10 - train loss/acc: 2.514 0.140, valid loss/acc: 2.512 0.140\n",
            "epoch 11 - train loss/acc: 2.499 0.145, valid loss/acc: 2.497 0.144\n",
            "epoch 12 - train loss/acc: 2.484 0.150, valid loss/acc: 2.483 0.148\n",
            "epoch 13 - train loss/acc: 2.470 0.154, valid loss/acc: 2.469 0.153\n",
            "epoch 14 - train loss/acc: 2.457 0.158, valid loss/acc: 2.456 0.156\n",
            "epoch 15 - train loss/acc: 2.444 0.161, valid loss/acc: 2.443 0.159\n",
            "epoch 16 - train loss/acc: 2.432 0.166, valid loss/acc: 2.431 0.164\n",
            "epoch 17 - train loss/acc: 2.420 0.170, valid loss/acc: 2.419 0.168\n",
            "epoch 18 - train loss/acc: 2.409 0.173, valid loss/acc: 2.408 0.172\n",
            "epoch 19 - train loss/acc: 2.398 0.177, valid loss/acc: 2.397 0.175\n",
            "epoch 20 - train loss/acc: 2.387 0.180, valid loss/acc: 2.386 0.179\n",
            "epoch 21 - train loss/acc: 2.377 0.183, valid loss/acc: 2.376 0.182\n",
            "epoch 22 - train loss/acc: 2.367 0.186, valid loss/acc: 2.366 0.186\n",
            "epoch 23 - train loss/acc: 2.358 0.190, valid loss/acc: 2.357 0.188\n",
            "epoch 24 - train loss/acc: 2.348 0.194, valid loss/acc: 2.348 0.191\n",
            "epoch 25 - train loss/acc: 2.339 0.197, valid loss/acc: 2.339 0.194\n",
            "epoch 26 - train loss/acc: 2.331 0.200, valid loss/acc: 2.330 0.198\n",
            "epoch 27 - train loss/acc: 2.322 0.203, valid loss/acc: 2.322 0.201\n",
            "epoch 28 - train loss/acc: 2.314 0.207, valid loss/acc: 2.314 0.204\n",
            "epoch 29 - train loss/acc: 2.306 0.211, valid loss/acc: 2.306 0.207\n",
            "epoch 30 - train loss/acc: 2.299 0.214, valid loss/acc: 2.298 0.210\n",
            "epoch 31 - train loss/acc: 2.291 0.218, valid loss/acc: 2.291 0.213\n",
            "epoch 32 - train loss/acc: 2.284 0.222, valid loss/acc: 2.284 0.217\n",
            "epoch 33 - train loss/acc: 2.277 0.225, valid loss/acc: 2.277 0.220\n",
            "epoch 34 - train loss/acc: 2.271 0.228, valid loss/acc: 2.271 0.224\n",
            "epoch 35 - train loss/acc: 2.264 0.232, valid loss/acc: 2.264 0.227\n",
            "epoch 36 - train loss/acc: 2.258 0.235, valid loss/acc: 2.258 0.231\n",
            "epoch 37 - train loss/acc: 2.252 0.238, valid loss/acc: 2.252 0.235\n",
            "epoch 38 - train loss/acc: 2.246 0.241, valid loss/acc: 2.246 0.238\n",
            "epoch 39 - train loss/acc: 2.240 0.244, valid loss/acc: 2.240 0.241\n",
            "epoch 40 - train loss/acc: 2.234 0.248, valid loss/acc: 2.234 0.244\n",
            "epoch 41 - train loss/acc: 2.229 0.250, valid loss/acc: 2.229 0.247\n",
            "epoch 42 - train loss/acc: 2.223 0.254, valid loss/acc: 2.224 0.251\n",
            "epoch 43 - train loss/acc: 2.218 0.257, valid loss/acc: 2.218 0.254\n",
            "epoch 44 - train loss/acc: 2.213 0.260, valid loss/acc: 2.213 0.258\n",
            "epoch 45 - train loss/acc: 2.208 0.263, valid loss/acc: 2.208 0.260\n",
            "epoch 46 - train loss/acc: 2.203 0.266, valid loss/acc: 2.204 0.263\n",
            "epoch 47 - train loss/acc: 2.198 0.270, valid loss/acc: 2.199 0.265\n",
            "epoch 48 - train loss/acc: 2.194 0.273, valid loss/acc: 2.194 0.269\n",
            "epoch 49 - train loss/acc: 2.189 0.276, valid loss/acc: 2.190 0.272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpPsAlXU0T_Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d6606f0-e548-4c43-fa1f-43747d5c9ea9"
      },
      "source": [
        "# evalute the model on test data\n",
        "Y_hat, _ = model.forward(X_test)\n",
        "test_loss = model.compute_loss(Y_test, Y_hat)\n",
        "test_acc = model.evaluate(Y_test, Y_hat)\n",
        "print(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final test loss = 2.181, acc = 0.284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hh1PZpk_g0I"
      },
      "source": [
        "# Extra Credit (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R0n6y9_AgXc"
      },
      "source": [
        "def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n",
        "    \"\"\"\n",
        "    initializes parameters with He Initialization.\n",
        "\n",
        "    Question (g)\n",
        "    - refer to https://paperswithcode.com/method/he-initialization for He initialization\n",
        "\n",
        "    Inputs\n",
        "    - input_dim\n",
        "    - num_hiddens\n",
        "    - num_classes\n",
        "    Returns\n",
        "    - params: a dictionary with the initialized parameters.\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "\n",
        "    W1 = np.random.randn(input_dim, num_hiddens) * np.sqrt(2 / input_dim)\n",
        "    b1 = np.zeros((1, num_hiddens))\n",
        "    W2 = np.random.randn(num_hiddens, num_classes) * np.sqrt(2 / num_hiddens)\n",
        "    b2 = np.zeros((1, num_classes))\n",
        "\n",
        "    params = {\n",
        "        \"W1\": W1,\n",
        "        \"b1\": b1,\n",
        "        \"W2\": W2,\n",
        "        \"b2\": b2\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def forward_relu(self, X):\n",
        "    \"\"\"\n",
        "    Defines and performs the feed forward step of a two-layer neural network.\n",
        "    Specifically, the network structue is given by\n",
        "\n",
        "        y = softmax(relu(X W1 + b1) W2 + b2)\n",
        "\n",
        "    where X is the input matrix of shape (N, D), y is the class distribution matrix\n",
        "    of shape (N, C), N is the number of examples (either the entire dataset or\n",
        "    a mini-batch), D is the feature dimensionality, and C is the number of classes.\n",
        "\n",
        "    Question (g)\n",
        "\n",
        "    Inputs\n",
        "        X: the input matrix of shape (N, D)\n",
        "\n",
        "    Returns\n",
        "        y: the output of the model\n",
        "        ff_dict: a dictionary containing all the fully connected units and activations.\n",
        "    \"\"\"\n",
        "    W1, b1 = self.params['W1'], self.params['b1']\n",
        "    W2, b2 = self.params['W2'], self.params['b2']\n",
        "\n",
        "    # 첫번째 레이어\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = np.maximum(0, Z1)  # ReLU\n",
        "\n",
        "    # 두번째 레이어\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    y = softmax(Z2)\n",
        "\n",
        "    ff_dict = {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'Y': y}\n",
        "\n",
        "    return y, ff_dict\n",
        "\n",
        "def backward_relu(self, X, Y, ff_dict):\n",
        "    \"\"\"\n",
        "    Performs backpropagation over the two-layer neural network, and returns\n",
        "    a dictionary of gradients of all model parameters.\n",
        "\n",
        "    Question (g)\n",
        "\n",
        "    Inputs:\n",
        "        - X: the input matrix of shape (B, D), where B is the number of examples\n",
        "            in a mini-batch, D is the feature dimensionality.\n",
        "        - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n",
        "            where B is the number of examples in a mini-batch, C is the number\n",
        "            of classes.\n",
        "        - ff_dict: the dictionary containing all the fully connected units and\n",
        "            activations.\n",
        "\n",
        "    Returns:\n",
        "        - grads: a dictionary containing the gradients of corresponding weights\n",
        "            and biases.\n",
        "    \"\"\"\n",
        "    A1 = ff_dict['A1']\n",
        "    Y_hat = ff_dict['Y']\n",
        "    B = X.shape[0]\n",
        "\n",
        "    dY = (Y_hat - Y) / B\n",
        "\n",
        "    dW2 = np.dot(A1.T, dY)\n",
        "    db2 = np.sum(dY, axis=0, keepdims=True)\n",
        "\n",
        "    dA1 = np.dot(dY, self.params['W2'].T)\n",
        "    dZ1 = dA1 * (A1 > 0)\n",
        "\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    grads = {\n",
        "        'W1': dW1,\n",
        "        'b1': db1,\n",
        "        'W2': dW2,\n",
        "        'b2': db2\n",
        "    }\n",
        "    return grads\n",
        "\n",
        "\n",
        "TwoLayerNNRelu = copy.copy(TwoLayerNN)\n",
        "TwoLayerNNRelu.initialize_parameters = initialize_parameters\n",
        "TwoLayerNNRelu.feed_forward = forward_relu\n",
        "TwoLayerNNRelu.back_propagate = backward_relu"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSXnk6y9vAZn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4044764-faec-48bb-fe4f-d0df082faf69"
      },
      "source": [
        "###\n",
        "# Question (g)\n",
        "# Tune the hyperparameters with validation data,\n",
        "# and print the results by running the lines below.\n",
        "###\n",
        "\n",
        "# 하이퍼파라미터 튜닝 실험 횟수 (Random Search 횟수)\n",
        "exp_num = 10\n",
        "for i in range(exp_num):\n",
        "\n",
        "  # 하이퍼파라미터 선언 및 범위 지정\n",
        "  num_hiddens = 2 ** np.random.randint(5, 9)\n",
        "  lr = 10 ** np.random.uniform(-6, -2)\n",
        "  n_epochs = 5 * np.random.randint(1,10) # 축소\n",
        "  batch_size = 2 ** np.random.randint(5, 9)\n",
        "\n",
        "  print(f'======= TRY {i} | num_hiddens: {num_hiddens}, lr: {lr:.4f}, n_epochs: {n_epochs}, b_size = {batch_size} =======')\n",
        "\n",
        "  # 모델 세팅\n",
        "  model_relu = TwoLayerNNRelu(input_dim=784, num_hiddens=num_hiddens, num_classes=10)\n",
        "\n",
        "  # 모델 학습 및 Val 데이터로 평가\n",
        "  history = model_relu.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)\n",
        "  print()\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= TRY 0 | num_hiddens: 32, lr: 0.0000, n_epochs: 45, b_size = 32 =======\n",
            "epoch 00 - train loss/acc: 2.411 0.099, valid loss/acc: 2.408 0.101\n",
            "epoch 01 - train loss/acc: 2.406 0.099, valid loss/acc: 2.403 0.101\n",
            "epoch 02 - train loss/acc: 2.401 0.100, valid loss/acc: 2.398 0.101\n",
            "epoch 03 - train loss/acc: 2.396 0.100, valid loss/acc: 2.394 0.102\n",
            "epoch 04 - train loss/acc: 2.392 0.101, valid loss/acc: 2.389 0.103\n",
            "epoch 05 - train loss/acc: 2.387 0.101, valid loss/acc: 2.385 0.103\n",
            "epoch 06 - train loss/acc: 2.383 0.102, valid loss/acc: 2.381 0.104\n",
            "epoch 07 - train loss/acc: 2.379 0.103, valid loss/acc: 2.377 0.105\n",
            "epoch 08 - train loss/acc: 2.374 0.105, valid loss/acc: 2.372 0.106\n",
            "epoch 09 - train loss/acc: 2.370 0.106, valid loss/acc: 2.368 0.108\n",
            "epoch 10 - train loss/acc: 2.366 0.107, valid loss/acc: 2.365 0.109\n",
            "epoch 11 - train loss/acc: 2.363 0.109, valid loss/acc: 2.361 0.111\n",
            "epoch 12 - train loss/acc: 2.359 0.111, valid loss/acc: 2.357 0.113\n",
            "epoch 13 - train loss/acc: 2.355 0.112, valid loss/acc: 2.354 0.114\n",
            "epoch 14 - train loss/acc: 2.351 0.113, valid loss/acc: 2.350 0.115\n",
            "epoch 15 - train loss/acc: 2.348 0.115, valid loss/acc: 2.346 0.117\n",
            "epoch 16 - train loss/acc: 2.344 0.116, valid loss/acc: 2.343 0.118\n",
            "epoch 17 - train loss/acc: 2.341 0.118, valid loss/acc: 2.340 0.120\n",
            "epoch 18 - train loss/acc: 2.338 0.120, valid loss/acc: 2.336 0.122\n",
            "epoch 19 - train loss/acc: 2.334 0.122, valid loss/acc: 2.333 0.123\n",
            "epoch 20 - train loss/acc: 2.331 0.124, valid loss/acc: 2.330 0.125\n",
            "epoch 21 - train loss/acc: 2.328 0.126, valid loss/acc: 2.327 0.127\n",
            "epoch 22 - train loss/acc: 2.325 0.127, valid loss/acc: 2.324 0.128\n",
            "epoch 23 - train loss/acc: 2.322 0.129, valid loss/acc: 2.321 0.130\n",
            "epoch 24 - train loss/acc: 2.319 0.130, valid loss/acc: 2.318 0.132\n",
            "epoch 25 - train loss/acc: 2.316 0.132, valid loss/acc: 2.315 0.135\n",
            "epoch 26 - train loss/acc: 2.313 0.134, valid loss/acc: 2.312 0.137\n",
            "epoch 27 - train loss/acc: 2.310 0.136, valid loss/acc: 2.310 0.138\n",
            "epoch 28 - train loss/acc: 2.307 0.137, valid loss/acc: 2.307 0.138\n",
            "epoch 29 - train loss/acc: 2.305 0.139, valid loss/acc: 2.304 0.141\n",
            "epoch 30 - train loss/acc: 2.302 0.141, valid loss/acc: 2.302 0.142\n",
            "epoch 31 - train loss/acc: 2.299 0.142, valid loss/acc: 2.299 0.144\n",
            "epoch 32 - train loss/acc: 2.297 0.144, valid loss/acc: 2.296 0.146\n",
            "epoch 33 - train loss/acc: 2.294 0.145, valid loss/acc: 2.294 0.146\n",
            "epoch 34 - train loss/acc: 2.292 0.147, valid loss/acc: 2.291 0.147\n",
            "epoch 35 - train loss/acc: 2.289 0.149, valid loss/acc: 2.289 0.149\n",
            "epoch 36 - train loss/acc: 2.287 0.151, valid loss/acc: 2.286 0.151\n",
            "epoch 37 - train loss/acc: 2.284 0.153, valid loss/acc: 2.284 0.153\n",
            "epoch 38 - train loss/acc: 2.282 0.155, valid loss/acc: 2.282 0.156\n",
            "epoch 39 - train loss/acc: 2.279 0.157, valid loss/acc: 2.279 0.158\n",
            "epoch 40 - train loss/acc: 2.277 0.159, valid loss/acc: 2.277 0.161\n",
            "epoch 41 - train loss/acc: 2.275 0.161, valid loss/acc: 2.275 0.164\n",
            "epoch 42 - train loss/acc: 2.272 0.163, valid loss/acc: 2.273 0.167\n",
            "epoch 43 - train loss/acc: 2.270 0.166, valid loss/acc: 2.270 0.169\n",
            "epoch 44 - train loss/acc: 2.268 0.168, valid loss/acc: 2.268 0.171\n",
            "\n",
            "======= TRY 1 | num_hiddens: 32, lr: 0.0008, n_epochs: 5, b_size = 256 =======\n",
            "epoch 00 - train loss/acc: 2.369 0.106, valid loss/acc: 2.367 0.108\n",
            "epoch 01 - train loss/acc: 2.332 0.123, valid loss/acc: 2.331 0.125\n",
            "epoch 02 - train loss/acc: 2.302 0.140, valid loss/acc: 2.302 0.142\n",
            "epoch 03 - train loss/acc: 2.276 0.160, valid loss/acc: 2.276 0.162\n",
            "epoch 04 - train loss/acc: 2.254 0.184, valid loss/acc: 2.254 0.185\n",
            "\n",
            "======= TRY 2 | num_hiddens: 64, lr: 0.0000, n_epochs: 30, b_size = 32 =======\n",
            "epoch 00 - train loss/acc: 2.467 0.113, valid loss/acc: 2.471 0.111\n",
            "epoch 01 - train loss/acc: 2.465 0.113, valid loss/acc: 2.469 0.111\n",
            "epoch 02 - train loss/acc: 2.462 0.113, valid loss/acc: 2.466 0.111\n",
            "epoch 03 - train loss/acc: 2.460 0.113, valid loss/acc: 2.464 0.111\n",
            "epoch 04 - train loss/acc: 2.458 0.113, valid loss/acc: 2.462 0.111\n",
            "epoch 05 - train loss/acc: 2.455 0.113, valid loss/acc: 2.459 0.111\n",
            "epoch 06 - train loss/acc: 2.453 0.113, valid loss/acc: 2.457 0.111\n",
            "epoch 07 - train loss/acc: 2.451 0.113, valid loss/acc: 2.455 0.111\n",
            "epoch 08 - train loss/acc: 2.449 0.113, valid loss/acc: 2.453 0.111\n",
            "epoch 09 - train loss/acc: 2.447 0.113, valid loss/acc: 2.451 0.111\n",
            "epoch 10 - train loss/acc: 2.445 0.113, valid loss/acc: 2.448 0.111\n",
            "epoch 11 - train loss/acc: 2.443 0.113, valid loss/acc: 2.446 0.111\n",
            "epoch 12 - train loss/acc: 2.440 0.113, valid loss/acc: 2.444 0.111\n",
            "epoch 13 - train loss/acc: 2.438 0.113, valid loss/acc: 2.442 0.111\n",
            "epoch 14 - train loss/acc: 2.436 0.113, valid loss/acc: 2.440 0.111\n",
            "epoch 15 - train loss/acc: 2.434 0.113, valid loss/acc: 2.438 0.111\n",
            "epoch 16 - train loss/acc: 2.433 0.113, valid loss/acc: 2.436 0.111\n",
            "epoch 17 - train loss/acc: 2.431 0.113, valid loss/acc: 2.434 0.111\n",
            "epoch 18 - train loss/acc: 2.429 0.113, valid loss/acc: 2.433 0.111\n",
            "epoch 19 - train loss/acc: 2.427 0.113, valid loss/acc: 2.431 0.111\n",
            "epoch 20 - train loss/acc: 2.425 0.113, valid loss/acc: 2.429 0.111\n",
            "epoch 21 - train loss/acc: 2.423 0.113, valid loss/acc: 2.427 0.111\n",
            "epoch 22 - train loss/acc: 2.421 0.113, valid loss/acc: 2.425 0.111\n",
            "epoch 23 - train loss/acc: 2.420 0.113, valid loss/acc: 2.423 0.111\n",
            "epoch 24 - train loss/acc: 2.418 0.113, valid loss/acc: 2.422 0.111\n",
            "epoch 25 - train loss/acc: 2.416 0.113, valid loss/acc: 2.420 0.111\n",
            "epoch 26 - train loss/acc: 2.414 0.113, valid loss/acc: 2.418 0.111\n",
            "epoch 27 - train loss/acc: 2.413 0.113, valid loss/acc: 2.417 0.111\n",
            "epoch 28 - train loss/acc: 2.411 0.113, valid loss/acc: 2.415 0.111\n",
            "epoch 29 - train loss/acc: 2.410 0.113, valid loss/acc: 2.413 0.111\n",
            "\n",
            "======= TRY 3 | num_hiddens: 256, lr: 0.0067, n_epochs: 35, b_size = 256 =======\n",
            "epoch 00 - train loss/acc: 2.103 0.462, valid loss/acc: 2.106 0.454\n",
            "epoch 01 - train loss/acc: 1.915 0.625, valid loss/acc: 1.920 0.615\n",
            "epoch 02 - train loss/acc: 1.747 0.690, valid loss/acc: 1.754 0.681\n",
            "epoch 03 - train loss/acc: 1.595 0.727, valid loss/acc: 1.604 0.715\n",
            "epoch 04 - train loss/acc: 1.461 0.750, valid loss/acc: 1.471 0.741\n",
            "epoch 05 - train loss/acc: 1.344 0.760, valid loss/acc: 1.355 0.753\n",
            "epoch 06 - train loss/acc: 1.241 0.778, valid loss/acc: 1.253 0.769\n",
            "epoch 07 - train loss/acc: 1.153 0.792, valid loss/acc: 1.165 0.781\n",
            "epoch 08 - train loss/acc: 1.077 0.803, valid loss/acc: 1.090 0.792\n",
            "epoch 09 - train loss/acc: 1.011 0.810, valid loss/acc: 1.024 0.800\n",
            "epoch 10 - train loss/acc: 0.954 0.817, valid loss/acc: 0.968 0.808\n",
            "epoch 11 - train loss/acc: 0.904 0.823, valid loss/acc: 0.918 0.816\n",
            "epoch 12 - train loss/acc: 0.861 0.830, valid loss/acc: 0.875 0.824\n",
            "epoch 13 - train loss/acc: 0.823 0.832, valid loss/acc: 0.837 0.825\n",
            "epoch 14 - train loss/acc: 0.790 0.838, valid loss/acc: 0.804 0.832\n",
            "epoch 15 - train loss/acc: 0.760 0.839, valid loss/acc: 0.774 0.833\n",
            "epoch 16 - train loss/acc: 0.733 0.844, valid loss/acc: 0.747 0.838\n",
            "epoch 17 - train loss/acc: 0.709 0.847, valid loss/acc: 0.723 0.843\n",
            "epoch 18 - train loss/acc: 0.687 0.849, valid loss/acc: 0.701 0.845\n",
            "epoch 19 - train loss/acc: 0.667 0.851, valid loss/acc: 0.681 0.847\n",
            "epoch 20 - train loss/acc: 0.650 0.853, valid loss/acc: 0.663 0.849\n",
            "epoch 21 - train loss/acc: 0.633 0.855, valid loss/acc: 0.646 0.853\n",
            "epoch 22 - train loss/acc: 0.618 0.859, valid loss/acc: 0.631 0.857\n",
            "epoch 23 - train loss/acc: 0.604 0.860, valid loss/acc: 0.617 0.858\n",
            "epoch 24 - train loss/acc: 0.591 0.862, valid loss/acc: 0.604 0.860\n",
            "epoch 25 - train loss/acc: 0.580 0.863, valid loss/acc: 0.592 0.862\n",
            "epoch 26 - train loss/acc: 0.568 0.865, valid loss/acc: 0.581 0.863\n",
            "epoch 27 - train loss/acc: 0.558 0.866, valid loss/acc: 0.571 0.864\n",
            "epoch 28 - train loss/acc: 0.549 0.867, valid loss/acc: 0.561 0.865\n",
            "epoch 29 - train loss/acc: 0.540 0.868, valid loss/acc: 0.552 0.866\n",
            "epoch 30 - train loss/acc: 0.531 0.870, valid loss/acc: 0.544 0.868\n",
            "epoch 31 - train loss/acc: 0.523 0.872, valid loss/acc: 0.535 0.870\n",
            "epoch 32 - train loss/acc: 0.516 0.873, valid loss/acc: 0.528 0.870\n",
            "epoch 33 - train loss/acc: 0.509 0.873, valid loss/acc: 0.521 0.872\n",
            "epoch 34 - train loss/acc: 0.502 0.874, valid loss/acc: 0.514 0.872\n",
            "\n",
            "======= TRY 4 | num_hiddens: 32, lr: 0.0001, n_epochs: 10, b_size = 64 =======\n",
            "epoch 00 - train loss/acc: 2.397 0.100, valid loss/acc: 2.395 0.102\n",
            "epoch 01 - train loss/acc: 2.380 0.103, valid loss/acc: 2.378 0.105\n",
            "epoch 02 - train loss/acc: 2.365 0.108, valid loss/acc: 2.363 0.110\n",
            "epoch 03 - train loss/acc: 2.351 0.113, valid loss/acc: 2.349 0.116\n",
            "epoch 04 - train loss/acc: 2.338 0.120, valid loss/acc: 2.337 0.122\n",
            "epoch 05 - train loss/acc: 2.326 0.127, valid loss/acc: 2.325 0.128\n",
            "epoch 06 - train loss/acc: 2.315 0.133, valid loss/acc: 2.314 0.136\n",
            "epoch 07 - train loss/acc: 2.304 0.139, valid loss/acc: 2.303 0.141\n",
            "epoch 08 - train loss/acc: 2.294 0.145, valid loss/acc: 2.294 0.146\n",
            "epoch 09 - train loss/acc: 2.284 0.153, valid loss/acc: 2.284 0.153\n",
            "\n",
            "======= TRY 5 | num_hiddens: 128, lr: 0.0000, n_epochs: 25, b_size = 64 =======\n",
            "epoch 00 - train loss/acc: 2.464 0.104, valid loss/acc: 2.462 0.105\n",
            "epoch 01 - train loss/acc: 2.446 0.104, valid loss/acc: 2.445 0.105\n",
            "epoch 02 - train loss/acc: 2.431 0.105, valid loss/acc: 2.430 0.106\n",
            "epoch 03 - train loss/acc: 2.417 0.106, valid loss/acc: 2.416 0.107\n",
            "epoch 04 - train loss/acc: 2.404 0.108, valid loss/acc: 2.403 0.109\n",
            "epoch 05 - train loss/acc: 2.392 0.111, valid loss/acc: 2.392 0.110\n",
            "epoch 06 - train loss/acc: 2.381 0.114, valid loss/acc: 2.381 0.114\n",
            "epoch 07 - train loss/acc: 2.371 0.119, valid loss/acc: 2.371 0.118\n",
            "epoch 08 - train loss/acc: 2.362 0.125, valid loss/acc: 2.361 0.123\n",
            "epoch 09 - train loss/acc: 2.353 0.132, valid loss/acc: 2.352 0.130\n",
            "epoch 10 - train loss/acc: 2.345 0.140, valid loss/acc: 2.344 0.139\n",
            "epoch 11 - train loss/acc: 2.337 0.149, valid loss/acc: 2.336 0.147\n",
            "epoch 12 - train loss/acc: 2.330 0.159, valid loss/acc: 2.329 0.157\n",
            "epoch 13 - train loss/acc: 2.323 0.169, valid loss/acc: 2.322 0.169\n",
            "epoch 14 - train loss/acc: 2.316 0.180, valid loss/acc: 2.316 0.179\n",
            "epoch 15 - train loss/acc: 2.310 0.190, valid loss/acc: 2.310 0.190\n",
            "epoch 16 - train loss/acc: 2.305 0.200, valid loss/acc: 2.304 0.201\n",
            "epoch 17 - train loss/acc: 2.299 0.210, valid loss/acc: 2.299 0.210\n",
            "epoch 18 - train loss/acc: 2.294 0.220, valid loss/acc: 2.294 0.218\n",
            "epoch 19 - train loss/acc: 2.289 0.229, valid loss/acc: 2.289 0.225\n",
            "epoch 20 - train loss/acc: 2.284 0.237, valid loss/acc: 2.284 0.231\n",
            "epoch 21 - train loss/acc: 2.280 0.243, valid loss/acc: 2.280 0.238\n",
            "epoch 22 - train loss/acc: 2.276 0.250, valid loss/acc: 2.276 0.245\n",
            "epoch 23 - train loss/acc: 2.272 0.256, valid loss/acc: 2.272 0.251\n",
            "epoch 24 - train loss/acc: 2.268 0.261, valid loss/acc: 2.268 0.257\n",
            "\n",
            "======= TRY 6 | num_hiddens: 256, lr: 0.0002, n_epochs: 40, b_size = 32 =======\n",
            "epoch 00 - train loss/acc: 2.252 0.281, valid loss/acc: 2.254 0.279\n",
            "epoch 01 - train loss/acc: 2.187 0.343, valid loss/acc: 2.189 0.338\n",
            "epoch 02 - train loss/acc: 2.127 0.446, valid loss/acc: 2.130 0.438\n",
            "epoch 03 - train loss/acc: 2.069 0.519, valid loss/acc: 2.072 0.506\n",
            "epoch 04 - train loss/acc: 2.013 0.565, valid loss/acc: 2.017 0.552\n",
            "epoch 05 - train loss/acc: 1.958 0.595, valid loss/acc: 1.963 0.585\n",
            "epoch 06 - train loss/acc: 1.906 0.624, valid loss/acc: 1.911 0.613\n",
            "epoch 07 - train loss/acc: 1.855 0.651, valid loss/acc: 1.861 0.639\n",
            "epoch 08 - train loss/acc: 1.805 0.667, valid loss/acc: 1.812 0.657\n",
            "epoch 09 - train loss/acc: 1.758 0.681, valid loss/acc: 1.765 0.672\n",
            "epoch 10 - train loss/acc: 1.711 0.694, valid loss/acc: 1.719 0.684\n",
            "epoch 11 - train loss/acc: 1.666 0.711, valid loss/acc: 1.674 0.701\n",
            "epoch 12 - train loss/acc: 1.623 0.721, valid loss/acc: 1.631 0.709\n",
            "epoch 13 - train loss/acc: 1.581 0.723, valid loss/acc: 1.590 0.712\n",
            "epoch 14 - train loss/acc: 1.540 0.731, valid loss/acc: 1.550 0.722\n",
            "epoch 15 - train loss/acc: 1.501 0.739, valid loss/acc: 1.511 0.729\n",
            "epoch 16 - train loss/acc: 1.464 0.746, valid loss/acc: 1.474 0.737\n",
            "epoch 17 - train loss/acc: 1.428 0.754, valid loss/acc: 1.438 0.745\n",
            "epoch 18 - train loss/acc: 1.393 0.758, valid loss/acc: 1.403 0.749\n",
            "epoch 19 - train loss/acc: 1.359 0.762, valid loss/acc: 1.370 0.753\n",
            "epoch 20 - train loss/acc: 1.327 0.767, valid loss/acc: 1.339 0.756\n",
            "epoch 21 - train loss/acc: 1.296 0.772, valid loss/acc: 1.308 0.763\n",
            "epoch 22 - train loss/acc: 1.267 0.779, valid loss/acc: 1.279 0.770\n",
            "epoch 23 - train loss/acc: 1.239 0.780, valid loss/acc: 1.251 0.771\n",
            "epoch 24 - train loss/acc: 1.211 0.785, valid loss/acc: 1.224 0.776\n",
            "epoch 25 - train loss/acc: 1.185 0.788, valid loss/acc: 1.198 0.778\n",
            "epoch 26 - train loss/acc: 1.161 0.792, valid loss/acc: 1.173 0.781\n",
            "epoch 27 - train loss/acc: 1.137 0.795, valid loss/acc: 1.149 0.785\n",
            "epoch 28 - train loss/acc: 1.114 0.797, valid loss/acc: 1.127 0.787\n",
            "epoch 29 - train loss/acc: 1.092 0.799, valid loss/acc: 1.105 0.788\n",
            "epoch 30 - train loss/acc: 1.071 0.804, valid loss/acc: 1.084 0.792\n",
            "epoch 31 - train loss/acc: 1.051 0.806, valid loss/acc: 1.064 0.796\n",
            "epoch 32 - train loss/acc: 1.032 0.809, valid loss/acc: 1.045 0.799\n",
            "epoch 33 - train loss/acc: 1.014 0.811, valid loss/acc: 1.027 0.801\n",
            "epoch 34 - train loss/acc: 0.996 0.812, valid loss/acc: 1.009 0.803\n",
            "epoch 35 - train loss/acc: 0.979 0.813, valid loss/acc: 0.993 0.805\n",
            "epoch 36 - train loss/acc: 0.963 0.816, valid loss/acc: 0.976 0.808\n",
            "epoch 37 - train loss/acc: 0.947 0.818, valid loss/acc: 0.961 0.810\n",
            "epoch 38 - train loss/acc: 0.932 0.820, valid loss/acc: 0.946 0.813\n",
            "epoch 39 - train loss/acc: 0.918 0.822, valid loss/acc: 0.932 0.814\n",
            "\n",
            "======= TRY 7 | num_hiddens: 256, lr: 0.0034, n_epochs: 5, b_size = 256 =======\n",
            "epoch 00 - train loss/acc: 2.202 0.323, valid loss/acc: 2.205 0.321\n",
            "epoch 01 - train loss/acc: 2.098 0.478, valid loss/acc: 2.101 0.468\n",
            "epoch 02 - train loss/acc: 2.000 0.584, valid loss/acc: 2.004 0.571\n",
            "epoch 03 - train loss/acc: 1.907 0.635, valid loss/acc: 1.912 0.623\n",
            "epoch 04 - train loss/acc: 1.819 0.667, valid loss/acc: 1.825 0.655\n",
            "\n",
            "======= TRY 8 | num_hiddens: 32, lr: 0.0002, n_epochs: 15, b_size = 256 =======\n",
            "epoch 00 - train loss/acc: 2.404 0.099, valid loss/acc: 2.401 0.101\n",
            "epoch 01 - train loss/acc: 2.392 0.101, valid loss/acc: 2.390 0.103\n",
            "epoch 02 - train loss/acc: 2.382 0.103, valid loss/acc: 2.379 0.104\n",
            "epoch 03 - train loss/acc: 2.371 0.106, valid loss/acc: 2.370 0.107\n",
            "epoch 04 - train loss/acc: 2.362 0.109, valid loss/acc: 2.360 0.111\n",
            "epoch 05 - train loss/acc: 2.353 0.113, valid loss/acc: 2.351 0.115\n",
            "epoch 06 - train loss/acc: 2.344 0.116, valid loss/acc: 2.343 0.118\n",
            "epoch 07 - train loss/acc: 2.336 0.121, valid loss/acc: 2.335 0.122\n",
            "epoch 08 - train loss/acc: 2.328 0.125, valid loss/acc: 2.327 0.127\n",
            "epoch 09 - train loss/acc: 2.321 0.129, valid loss/acc: 2.320 0.131\n",
            "epoch 10 - train loss/acc: 2.314 0.134, valid loss/acc: 2.313 0.137\n",
            "epoch 11 - train loss/acc: 2.307 0.137, valid loss/acc: 2.306 0.139\n",
            "epoch 12 - train loss/acc: 2.300 0.142, valid loss/acc: 2.300 0.143\n",
            "epoch 13 - train loss/acc: 2.294 0.145, valid loss/acc: 2.294 0.146\n",
            "epoch 14 - train loss/acc: 2.288 0.150, valid loss/acc: 2.288 0.150\n",
            "\n",
            "======= TRY 9 | num_hiddens: 32, lr: 0.0000, n_epochs: 25, b_size = 64 =======\n",
            "epoch 00 - train loss/acc: 2.414 0.099, valid loss/acc: 2.412 0.101\n",
            "epoch 01 - train loss/acc: 2.413 0.099, valid loss/acc: 2.410 0.101\n",
            "epoch 02 - train loss/acc: 2.412 0.099, valid loss/acc: 2.409 0.101\n",
            "epoch 03 - train loss/acc: 2.410 0.099, valid loss/acc: 2.408 0.101\n",
            "epoch 04 - train loss/acc: 2.409 0.099, valid loss/acc: 2.406 0.101\n",
            "epoch 05 - train loss/acc: 2.408 0.099, valid loss/acc: 2.405 0.101\n",
            "epoch 06 - train loss/acc: 2.407 0.099, valid loss/acc: 2.404 0.101\n",
            "epoch 07 - train loss/acc: 2.405 0.099, valid loss/acc: 2.403 0.101\n",
            "epoch 08 - train loss/acc: 2.404 0.099, valid loss/acc: 2.401 0.101\n",
            "epoch 09 - train loss/acc: 2.403 0.100, valid loss/acc: 2.400 0.101\n",
            "epoch 10 - train loss/acc: 2.401 0.100, valid loss/acc: 2.399 0.101\n",
            "epoch 11 - train loss/acc: 2.400 0.100, valid loss/acc: 2.398 0.102\n",
            "epoch 12 - train loss/acc: 2.399 0.100, valid loss/acc: 2.396 0.102\n",
            "epoch 13 - train loss/acc: 2.398 0.100, valid loss/acc: 2.395 0.102\n",
            "epoch 14 - train loss/acc: 2.396 0.100, valid loss/acc: 2.394 0.102\n",
            "epoch 15 - train loss/acc: 2.395 0.100, valid loss/acc: 2.393 0.102\n",
            "epoch 16 - train loss/acc: 2.394 0.101, valid loss/acc: 2.392 0.102\n",
            "epoch 17 - train loss/acc: 2.393 0.101, valid loss/acc: 2.391 0.103\n",
            "epoch 18 - train loss/acc: 2.392 0.101, valid loss/acc: 2.389 0.103\n",
            "epoch 19 - train loss/acc: 2.390 0.101, valid loss/acc: 2.388 0.103\n",
            "epoch 20 - train loss/acc: 2.389 0.101, valid loss/acc: 2.387 0.103\n",
            "epoch 21 - train loss/acc: 2.388 0.101, valid loss/acc: 2.386 0.103\n",
            "epoch 22 - train loss/acc: 2.387 0.101, valid loss/acc: 2.385 0.103\n",
            "epoch 23 - train loss/acc: 2.386 0.102, valid loss/acc: 2.384 0.103\n",
            "epoch 24 - train loss/acc: 2.385 0.102, valid loss/acc: 2.382 0.104\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRY3 의 조합으로 최종 테스트를 수행해보았다."
      ],
      "metadata": {
        "id": "CJ9T-CximKKf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-jJRXqsBxzh"
      },
      "source": [
        "# model instantiation\n",
        "model_relu = TwoLayerNNRelu(input_dim=784, num_hiddens=256, num_classes=10)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC8f80a0w53m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "187bf83d-937e-49b0-f489-06ca815936e3"
      },
      "source": [
        "# train the model & evaluate with validation data\n",
        "lr, n_epochs, batch_size = 0.0067, 35, 256\n",
        "history = model_relu.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 00 - train loss/acc: 2.104 0.461, valid loss/acc: 2.107 0.453\n",
            "epoch 01 - train loss/acc: 1.917 0.624, valid loss/acc: 1.922 0.614\n",
            "epoch 02 - train loss/acc: 1.749 0.689, valid loss/acc: 1.756 0.680\n",
            "epoch 03 - train loss/acc: 1.598 0.726, valid loss/acc: 1.607 0.715\n",
            "epoch 04 - train loss/acc: 1.464 0.750, valid loss/acc: 1.474 0.741\n",
            "epoch 05 - train loss/acc: 1.347 0.760, valid loss/acc: 1.358 0.752\n",
            "epoch 06 - train loss/acc: 1.244 0.777, valid loss/acc: 1.256 0.768\n",
            "epoch 07 - train loss/acc: 1.156 0.791, valid loss/acc: 1.169 0.780\n",
            "epoch 08 - train loss/acc: 1.080 0.802, valid loss/acc: 1.093 0.792\n",
            "epoch 09 - train loss/acc: 1.014 0.810, valid loss/acc: 1.027 0.800\n",
            "epoch 10 - train loss/acc: 0.957 0.816, valid loss/acc: 0.970 0.808\n",
            "epoch 11 - train loss/acc: 0.907 0.823, valid loss/acc: 0.921 0.816\n",
            "epoch 12 - train loss/acc: 0.864 0.830, valid loss/acc: 0.877 0.824\n",
            "epoch 13 - train loss/acc: 0.826 0.832, valid loss/acc: 0.839 0.825\n",
            "epoch 14 - train loss/acc: 0.792 0.838, valid loss/acc: 0.806 0.831\n",
            "epoch 15 - train loss/acc: 0.762 0.839, valid loss/acc: 0.776 0.832\n",
            "epoch 16 - train loss/acc: 0.735 0.844, valid loss/acc: 0.749 0.837\n",
            "epoch 17 - train loss/acc: 0.711 0.846, valid loss/acc: 0.725 0.842\n",
            "epoch 18 - train loss/acc: 0.689 0.849, valid loss/acc: 0.703 0.845\n",
            "epoch 19 - train loss/acc: 0.669 0.851, valid loss/acc: 0.683 0.847\n",
            "epoch 20 - train loss/acc: 0.651 0.853, valid loss/acc: 0.665 0.849\n",
            "epoch 21 - train loss/acc: 0.635 0.855, valid loss/acc: 0.648 0.853\n",
            "epoch 22 - train loss/acc: 0.620 0.858, valid loss/acc: 0.633 0.857\n",
            "epoch 23 - train loss/acc: 0.606 0.859, valid loss/acc: 0.619 0.858\n",
            "epoch 24 - train loss/acc: 0.593 0.861, valid loss/acc: 0.606 0.859\n",
            "epoch 25 - train loss/acc: 0.581 0.863, valid loss/acc: 0.594 0.862\n",
            "epoch 26 - train loss/acc: 0.570 0.864, valid loss/acc: 0.583 0.862\n",
            "epoch 27 - train loss/acc: 0.560 0.866, valid loss/acc: 0.572 0.863\n",
            "epoch 28 - train loss/acc: 0.550 0.867, valid loss/acc: 0.562 0.865\n",
            "epoch 29 - train loss/acc: 0.541 0.868, valid loss/acc: 0.553 0.866\n",
            "epoch 30 - train loss/acc: 0.532 0.870, valid loss/acc: 0.545 0.868\n",
            "epoch 31 - train loss/acc: 0.524 0.871, valid loss/acc: 0.537 0.870\n",
            "epoch 32 - train loss/acc: 0.517 0.872, valid loss/acc: 0.529 0.870\n",
            "epoch 33 - train loss/acc: 0.510 0.873, valid loss/acc: 0.522 0.872\n",
            "epoch 34 - train loss/acc: 0.503 0.874, valid loss/acc: 0.515 0.872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i__6TfpCqOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd064ba9-4a41-4b89-865d-66b3cabc24d6"
      },
      "source": [
        "Y_hat, _ = model_relu.forward(X_test)\n",
        "test_loss = model_relu.compute_loss(Y_test, Y_hat)\n",
        "test_acc = model_relu.evaluate(Y_test, Y_hat)\n",
        "print(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final test loss = 0.486, acc = 0.882\n"
          ]
        }
      ]
    }
  ]
}